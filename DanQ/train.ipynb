{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import gzip\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, precision_recall_curve,\n",
    "    roc_auc_score, roc_curve,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from utils.pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://github.com/FunctionLab/selene/blob/master/models/danQ.py\n",
    "class DanQ(nn.Module):\n",
    "    \"\"\"DanQ architecture (Quang & Xie, 2016).\"\"\"\n",
    "\n",
    "    def __init__(self, sequence_length, n_features):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        sequence_length : int\n",
    "            Input sequence length\n",
    "        n_features : int\n",
    "            Total number of features to predict\n",
    "        \"\"\"\n",
    "        super(DanQ, self).__init__()\n",
    "\n",
    "        self.nnet = nn.Sequential(\n",
    "            nn.Conv1d(4, 320, kernel_size=26),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=13, stride=13),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.bdlstm = nn.Sequential(\n",
    "            nn.LSTM(320, 320, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        )\n",
    "\n",
    "        self._n_channels = math.floor((sequence_length - 25) / 13)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self._n_channels * 640, 925),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(925, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward propagation of a batch.\"\"\"\n",
    "        out = self.nnet(x)\n",
    "        reshape_out = out.transpose(0, 1).transpose(0, 2)\n",
    "        out, _ = self.bdlstm(reshape_out)\n",
    "        out = out.transpose(0, 1)\n",
    "        reshape_out = out.contiguous().view(\n",
    "            out.size(0), 640 * self._n_channels)\n",
    "        predict = self.classifier(reshape_out)\n",
    "\n",
    "        return(predict)\n",
    "\n",
    "def get_criterion():\n",
    "    \"\"\"\n",
    "    Specify the appropriate loss function (criterion) for this model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn._Loss\n",
    "    \"\"\"\n",
    "    return(nn.BCELoss())\n",
    "\n",
    "def get_optimizer(params, lr=0.001):\n",
    "    return(torch.optim.Adam(params, lr=lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(seq):\n",
    "    \"\"\"One hot encodes a sequence.\"\"\"\n",
    "\n",
    "    seq = seq.replace(\"A\", \"0\")\n",
    "    seq = seq.replace(\"C\", \"1\")\n",
    "    seq = seq.replace(\"G\", \"2\")\n",
    "    seq = seq.replace(\"T\", \"3\")\n",
    "\n",
    "    encoded_seq = np.zeros((4, len(seq)), dtype=\"float16\")\n",
    "\n",
    "    for i in range(len(seq)):\n",
    "        if seq[i].isdigit():\n",
    "            encoded_seq[int(seq[i]), i] = 1\n",
    "        else:\n",
    "            # i.e. Ns\n",
    "            encoded_seq[:, i] = 0.25\n",
    "\n",
    "    return(encoded_seq)\n",
    "\n",
    "def one_hot_decode(encoded_seq):\n",
    "    \"\"\"Reverts a sequence's one hot encoding.\"\"\"\n",
    "\n",
    "    seq = []\n",
    "    code = list(\"ACGT\")\n",
    " \n",
    "    for i in encoded_seq.transpose(1, 0):\n",
    "        try:\n",
    "            seq.append(code[int(np.where(i == 1)[0])])\n",
    "        except:\n",
    "            # i.e. N?\n",
    "            seq.append(\"N\")\n",
    "\n",
    "    return(\"\".join(seq))\n",
    "\n",
    "def reverse_complement(encoded_seqs):\n",
    "    \"\"\"Reverse complements one hot encoding for a list of sequences.\"\"\"\n",
    "    return(encoded_seqs[..., ::-1, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7          GGACAGGTCAACTTGAGGAGATTTTGGGCCTTCATAGGCCACCAGG...\n",
       "16         CCACATTATACAGCTTCTGAAAGGGTTGCTTGACCCACAGATGTGA...\n",
       "22         GAAGGAGACTGATGTGGTTTCTCCTCAGTTTCTCTGTGCGGCACCA...\n",
       "49         ACCTCTATGGTGTCGGCGAAGACCCGCCCTTGTGACGTCACGGAAG...\n",
       "107        GGGAATGCTAAACAGAGGCAGATCTAAACTTAGGAGTTAGGCTTCT...\n",
       "                                 ...                        \n",
       "1817711    TGCTAGGAGCCGCAGTCATACTGGCTGTGCATGAGACCATCCACCT...\n",
       "1817721    AAGGCAAAGTGAGAAAAAGAGGAAACTAGAAGGCTGGTTGGGCTGT...\n",
       "1817723    CCTTGTCTTGGCATTTTCGGAGAGAACATGGACTCTGTGTTGTTTG...\n",
       "1817732    CTCTTACTCTTTCTGTGTGTGAAATGTGCAAGTAGCTTTACAGTCT...\n",
       "1817832    TCTTCTTTCCCTTTCCCTCCTCCCTAGGGGGTGTGACTGTAGAGCA...\n",
       "Length: 78983, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse FASTA sequences\n",
    "pos_seqs = {}\n",
    "neg_seqs = {}\n",
    "with gzip.open(\"../Data/pos_seqs.fa.gz\", \"rt\") as handle:\n",
    "    for seq_record in SeqIO.parse(handle, \"fasta\"):\n",
    "        pos_seqs[seq_record.id] = str(seq_record.seq).upper()\n",
    "pos_seqs = pd.Series(pos_seqs)\n",
    "with gzip.open(\"../Data/neg_seqs.fa.gz\", \"rt\") as handle:\n",
    "    for seq_record in SeqIO.parse(handle, \"fasta\"):\n",
    "        neg_seqs[seq_record.id] = str(seq_record.seq).upper()\n",
    "neg_seqs = pd.Series(neg_seqs)\n",
    "pos_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 1., 1., ..., 1., 1., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 1., 1., 1.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 1., 0., ..., 1., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 1., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 1., ..., 0., 0., 1.]]], dtype=float16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode sequences\n",
    "pos_seqs_1_hot = pos_seqs.map(lambda x: one_hot_encode(x))\n",
    "pos_seqs_1_hot = np.stack(pos_seqs_1_hot, axis=0)\n",
    "neg_seqs_1_hot = neg_seqs.map(lambda x: one_hot_encode(x))\n",
    "neg_seqs_1_hot = np.stack(neg_seqs_1_hot, axis=0)\n",
    "pos_seqs_1_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 1., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 1., ..., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 1., 1.],\n",
       "        [1., 0., 1., ..., 1., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 1., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 1., 1.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 1., 1., ..., 0., 0., 1.],\n",
       "        [1., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[1., 0., 1., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]]], dtype=float16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split sequences into train, validation and test\n",
    "seed = 123\n",
    "pos_train_seqs, pos_test_seqs = train_test_split(\n",
    "    pos_seqs_1_hot, test_size=0.2, random_state=seed\n",
    ")\n",
    "pos_validation_seqs, pos_test_seqs = train_test_split(\n",
    "    pos_test_seqs, test_size=0.5, random_state=seed\n",
    ")\n",
    "neg_train_seqs, neg_test_seqs = train_test_split(\n",
    "    neg_seqs_1_hot, test_size=0.2, random_state=seed\n",
    ")\n",
    "neg_validation_seqs, neg_test_seqs = train_test_split(\n",
    "    neg_test_seqs, test_size=0.5, random_state=seed\n",
    ")\n",
    "pos_train_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7f2649adf190>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create TensorDatasets\n",
    "X = np.concatenate((pos_train_seqs, neg_train_seqs))\n",
    "y = np.concatenate(\n",
    "    (np.ones((len(pos_train_seqs), 1)), np.zeros((len(neg_train_seqs), 1)))\n",
    ")\n",
    "train_dataset = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "X = np.concatenate((pos_validation_seqs, neg_validation_seqs))\n",
    "y = np.concatenate(\n",
    "    (np.ones((len(pos_validation_seqs), 1)), np.zeros((len(neg_validation_seqs), 1)))\n",
    ")\n",
    "validation_dataset = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "X = np.concatenate((pos_test_seqs, neg_test_seqs))\n",
    "y = np.concatenate(\n",
    "    (np.ones((len(pos_test_seqs), 1)), np.zeros((len(neg_test_seqs), 1)))\n",
    ")\n",
    "test_dataset = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f2675799910>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "parameters = dict(batch_size=64, shuffle=True, num_workers=8)\n",
    "train_dataloader = DataLoader(train_dataset, **parameters)\n",
    "validation_dataloader = DataLoader(validation_dataset, **parameters)\n",
    "test_dataloader = DataLoader(test_dataset, **parameters)\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/100] train_loss: 0.46951 (90.089 sec) valid_loss: 0.31048 (3.426 sec)\n",
      "Validation loss decreased (inf --> 0.310475), saving model ...\n",
      "[  2/100] train_loss: 0.29060 (90.471 sec) valid_loss: 0.27290 (3.425 sec)\n",
      "Validation loss decreased (0.310475 --> 0.272897), saving model ...\n",
      "[  3/100] train_loss: 0.26428 (90.145 sec) valid_loss: 0.26039 (3.427 sec)\n",
      "Validation loss decreased (0.272897 --> 0.260393), saving model ...\n",
      "[  4/100] train_loss: 0.24563 (90.187 sec) valid_loss: 0.25642 (3.418 sec)\n",
      "Validation loss decreased (0.260393 --> 0.256423), saving model ...\n",
      "[  5/100] train_loss: 0.23120 (90.148 sec) valid_loss: 0.25111 (3.423 sec)\n",
      "Validation loss decreased (0.256423 --> 0.251108), saving model ...\n",
      "[  6/100] train_loss: 0.21669 (90.076 sec) valid_loss: 0.25783 (3.423 sec)\n",
      "EarlyStopping counter: 1 out of 10\n",
      "[  7/100] train_loss: 0.20354 (89.932 sec) valid_loss: 0.25682 (3.429 sec)\n",
      "EarlyStopping counter: 2 out of 10\n",
      "[  8/100] train_loss: 0.19187 (89.776 sec) valid_loss: 0.25954 (3.429 sec)\n",
      "EarlyStopping counter: 3 out of 10\n",
      "[  9/100] train_loss: 0.18090 (89.910 sec) valid_loss: 0.27503 (3.421 sec)\n",
      "EarlyStopping counter: 4 out of 10\n",
      "[ 10/100] train_loss: 0.17181 (90.489 sec) valid_loss: 0.27686 (3.433 sec)\n",
      "EarlyStopping counter: 5 out of 10\n",
      "[ 11/100] train_loss: 0.16224 (90.051 sec) valid_loss: 0.28054 (3.435 sec)\n",
      "EarlyStopping counter: 6 out of 10\n",
      "[ 12/100] train_loss: 0.15189 (90.500 sec) valid_loss: 0.28613 (3.431 sec)\n",
      "EarlyStopping counter: 7 out of 10\n",
      "[ 13/100] train_loss: 0.14454 (90.171 sec) valid_loss: 0.29219 (3.440 sec)\n",
      "EarlyStopping counter: 8 out of 10\n",
      "[ 14/100] train_loss: 0.13830 (90.222 sec) valid_loss: 0.29063 (3.439 sec)\n",
      "EarlyStopping counter: 9 out of 10\n",
      "[ 15/100] train_loss: 0.13105 (90.172 sec) valid_loss: 0.32193 (3.427 sec)\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Stop!!!\n"
     ]
    }
   ],
   "source": [
    "# Train and validate\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DanQ(len(pos_seqs[0]), 1).to(device)\n",
    "output_dir = \"./CTCF/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "losses_file = os.path.join(output_dir, \"losses.csv\")\n",
    "state_dict = os.path.join(output_dir, \"model.pth.tar\")\n",
    "\n",
    "if not os.path.exists(losses_file):\n",
    "\n",
    "    max_epochs = 100\n",
    "    criterion = get_criterion()\n",
    "    optimizer = get_optimizer(model.parameters(), lr=0.0003)\n",
    "    # scheduler = ReduceLROnPlateau(optimizer, \"min\", patience=5, verbose=True, factor=0.5)\n",
    "    early_stopping = EarlyStopping(10, True, path=state_dict)\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "\n",
    "        # Train\n",
    "        t_time = time()\n",
    "        model.train() # set the model in train mode\n",
    "        train_losses.append([])\n",
    "        for seqs, labels in train_dataloader:\n",
    "            x = seqs.to(device) # shape = (batch_size, 4, 200)\n",
    "            labels = labels.to(device)\n",
    "            # Zero existing gradients so they don't add up\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, labels) \n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Keep the loss\n",
    "            train_losses[-1].append(loss.item())\n",
    "        t_loss = np.average(train_losses[-1])\n",
    "        t_time = time() - t_time\n",
    "\n",
    "        # Validate\n",
    "        v_time = time()\n",
    "        model.eval() # set the model in evaluation mode\n",
    "        validation_losses.append([])\n",
    "        for seqs, labels in validation_dataloader:\n",
    "            x = seqs.to(device) # shape = (batch_size, 4, 200)\n",
    "            labels = labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                # Forward pass\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, labels) \n",
    "                # Keep the loss\n",
    "                validation_losses[-1].append(loss.item())\n",
    "        v_loss = np.average(validation_losses[-1])\n",
    "        v_time = time() - v_time\n",
    "\n",
    "        print(f'[{epoch:>{3}}/{max_epochs:>{3}}] '\n",
    "             +f'train_loss: {t_loss:.5f} ({t_time:.3f} sec) '\n",
    "             +f'valid_loss: {v_loss:.5f} ({v_time:.3f} sec)')\n",
    "\n",
    "        # Adjust learning rate\n",
    "        # scheduler.step(math.ceil(v_loss * 1000.0) / 1000.0)\n",
    "\n",
    "        # EarlyStopping needs to check if the validation loss has decresed, \n",
    "        # and if it has, it will save the current model\n",
    "        early_stopping(v_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Stop!!!\")\n",
    "            break\n",
    "\n",
    "    # Losses to DataFrame\n",
    "    data = []\n",
    "    for i in range(len(train_losses)):\n",
    "        for j in range(len(train_losses[i])):\n",
    "            data.append([\"train\", i+1, j+1, train_losses[i][j]])\n",
    "    for i in range(len(validation_losses)):\n",
    "        for j in range(len(validation_losses[i])):\n",
    "            data.append(\n",
    "                [\"validation\", i+1, j+1, validation_losses[i][j]]\n",
    "            )\n",
    "    df = pd.DataFrame(data, columns=[\"Mode\", \"Epoch\", \"Batch\", \"Loss\"])\n",
    "\n",
    "    # Save losses\n",
    "    df.to_csv(losses_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses\n",
    "df = pd.read_csv(losses_file, index_col=0)\n",
    "# Seaborn aesthetics\n",
    "sns.set_context(\"paper\", font_scale=1.5, rc={\"lines.linewidth\": 1.5})\n",
    "sns.set_palette(sns.color_palette([\"#1965B0\", \"#DC050C\"]))\n",
    "# Plot losses\n",
    "#kwargs = dict(estimator=None, ci=None)\n",
    "g = sns.lineplot(x=\"Epoch\", y=\"Loss\", hue=\"Mode\", data=df)\n",
    "# Plot best epoch (i.e. lowest validation loss)\n",
    "best_epoch = df[(df.Mode == \"validation\")][[\"Epoch\", \"Loss\"]]\\\n",
    "    .groupby(\"Epoch\").mean().idxmin()\n",
    "g.axvline(\n",
    "    int(best_epoch), linestyle=\":\", color=\"dimgray\", label=\"best epoch\"\n",
    ")\n",
    "# Plot legend\n",
    "g.legend_.remove()\n",
    "handles, labels = g.axes.get_legend_handles_labels()\n",
    "plt.legend(handles, labels, frameon=False)\n",
    "# Modify axes\n",
    "g.set(xlim=(0, int(df[\"Epoch\"].max()) + 1))\n",
    "g.set(ylim=(0, 0.5))\n",
    "# Remove spines\n",
    "sns.despine()\n",
    "# Save & close\n",
    "fig = g.get_figure()\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(output_dir, \"losses.png\"))\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97409576],\n",
       "       [0.01435998],\n",
       "       [0.4332375 ],\n",
       "       ...,\n",
       "       [0.9999329 ],\n",
       "       [0.05632421],\n",
       "       [0.99999857]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "y = None\n",
    "predictions = None\n",
    "model.load_state_dict(torch.load(state_dict))\n",
    "\n",
    "model.eval() # set the model in evaluation mode\n",
    "for seqs, labels in test_dataloader:\n",
    "    x = seqs.to(device) # shape = (batch_size, 4, 200)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "        # Save predictions\n",
    "        if predictions is None and y is None:\n",
    "            predictions = outputs.data.cpu().numpy()\n",
    "            y = labels.data.cpu().numpy()\n",
    "        else:\n",
    "            predictions = np.append(\n",
    "                predictions, outputs.data.cpu().numpy(), axis=0\n",
    "            )\n",
    "            y = np.append(y, labels.data.cpu().numpy(), axis=0)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final performance metrics: AUCROC: 0.96213, AUCPR: 0.96623, MCC: 0.81622\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "metrics = dict(AUCPR=None, AUCROC=None, MCC=None)\n",
    "p = predictions.flatten()\n",
    "l = y.flatten()\n",
    "\n",
    "# Metrics to DataFrame\n",
    "for metric in metrics:\n",
    "    if metric == \"AUCPR\":\n",
    "        score = average_precision_score(l, p)\n",
    "    elif metric == \"AUCROC\":\n",
    "        score = roc_auc_score(l, p)\n",
    "    elif metric == \"MCC\":\n",
    "        score = matthews_corrcoef(l, np.rint(p))\n",
    "    metrics[metric] = score\n",
    "\n",
    "print(f'Final performance metrics: '\n",
    "     +f'AUCROC: {metrics[\"AUCROC\"]:.5f}, '\n",
    "     +f'AUCPR: {metrics[\"AUCPR\"]:.5f}, '\n",
    "     +f'MCC: {metrics[\"MCC\"]:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "def __plot(data, columns, metric, score):\n",
    "\n",
    "    # Metric to DataFrame\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    # Seaborn aesthetics\n",
    "    sns.set_context(\"paper\", font_scale=1.5, rc={\"lines.linewidth\": 1.5})\n",
    "    sns.set_palette(sns.color_palette([\"#1965B0\"]))\n",
    "    # Plot metric\n",
    "    kwargs = dict(estimator=None, ci=None)\n",
    "    g = sns.lineplot(x=columns[0], y=columns[1], data=df, **kwargs)\n",
    "    # Add metric score\n",
    "    kwargs = dict(horizontalalignment=\"center\", verticalalignment=\"center\")\n",
    "    plt.text(.5, 0, \"%s = %.5f\" % (metric, score), **kwargs)\n",
    "    # Remove spines\n",
    "    sns.despine()\n",
    "    # Save & close\n",
    "    fig = g.get_figure()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(output_dir, \"%s.png\" % metric))\n",
    "    plt.close(fig)\n",
    "\n",
    "# AUCROC\n",
    "fpr, tpr, _ = roc_curve(l, p)\n",
    "data = list(zip(fpr, tpr))\n",
    "__plot(data, [\"Fpr\", \"Tpr\"], \"AUCROC\", metrics[\"AUCROC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUCPR\n",
    "prec, recall, _ = precision_recall_curve(l, p)\n",
    "# i.e. precision = 0, recall = 1\n",
    "prec = np.insert(prec, 0, 0., axis=0)\n",
    "recall = np.insert(recall, 0, 1., axis=0)\n",
    "data = list(zip(recall, prec))\n",
    "__plot(data, [\"Recall\", \"Precision\"], \"AUCPR\", metrics[\"AUCPR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7f25e8a7d130>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check:\n",
    "# Reverse complement test sequences\n",
    "pos_test_seqs_rc = reverse_complement(pos_test_seqs)\n",
    "neg_test_seqs_rc = reverse_complement(neg_test_seqs)\n",
    "# Create TensorDatasets\n",
    "X = np.concatenate((pos_test_seqs_rc, neg_test_seqs_rc))\n",
    "y = np.concatenate(\n",
    "    (np.ones((len(pos_test_seqs_rc), 1)), np.zeros((len(neg_test_seqs_rc), 1)))\n",
    ")\n",
    "test_dataset_rc = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "test_dataset_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f25e8c8da00>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "parameters = dict(batch_size=64, shuffle=True, num_workers=8)\n",
    "test_dataloader_rc = DataLoader(test_dataset_rc, **parameters)\n",
    "test_dataloader_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9998809 ],\n",
       "       [0.09347926],\n",
       "       [0.97753906],\n",
       "       ...,\n",
       "       [0.99864525],\n",
       "       [0.9995192 ],\n",
       "       [0.96260846]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test reverse complement sequences\n",
    "y = None\n",
    "predictions = None\n",
    "state_dict = os.path.join(output_dir, \"model.pth.tar\")\n",
    "model.load_state_dict(torch.load(state_dict))\n",
    "\n",
    "model.eval() # set the model in evaluation mode\n",
    "for seqs, labels in test_dataloader_rc:\n",
    "    x = seqs.to(device) # shape = (batch_size, 4, 200)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "        # Save predictions\n",
    "        if predictions is None and y is None:\n",
    "            predictions = outputs.data.cpu().numpy()\n",
    "            y = labels.data.cpu().numpy()\n",
    "        else:\n",
    "            predictions = np.append(\n",
    "                predictions, outputs.data.cpu().numpy(), axis=0\n",
    "            )\n",
    "            y = np.append(y, labels.data.cpu().numpy(), axis=0)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final performance metrics: AUCROC: 0.96119, AUCPR: 0.96613, MCC: 0.81351\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "metrics = dict(AUCPR=None, AUCROC=None, MCC=None)\n",
    "p = predictions.flatten()\n",
    "l = y.flatten()\n",
    "\n",
    "# Metrics to DataFrame\n",
    "for metric in metrics:\n",
    "    if metric == \"AUCPR\":\n",
    "        score = average_precision_score(l, p)\n",
    "    elif metric == \"AUCROC\":\n",
    "        score = roc_auc_score(l, p)\n",
    "    elif metric == \"MCC\":\n",
    "        score = matthews_corrcoef(l, np.rint(p))\n",
    "    metrics[metric] = score\n",
    "\n",
    "print(f'Final performance metrics: '\n",
    "     +f'AUCROC: {metrics[\"AUCROC\"]:.5f}, '\n",
    "     +f'AUCPR: {metrics[\"AUCPR\"]:.5f}, '\n",
    "     +f'MCC: {metrics[\"MCC\"]:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
