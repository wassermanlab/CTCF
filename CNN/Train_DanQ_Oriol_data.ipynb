{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "from torch import relu, sigmoid\n",
    "import torch.nn.modules.activation as activation\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to train a model - no validation\n",
    "def train_model(train_loader, model, device, criterion, optimizer, num_epochs,\n",
    "               weights_folder, name_ind, verbose):\n",
    "    total_step = len(train_loader)\n",
    "    train_error = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() #tell model explicitly that we train\n",
    "        logs = {}\n",
    "        running_loss = 0.0\n",
    "        for seqs, labels in train_loader:\n",
    "            x = seqs.to(device) #the input here is (batch_size, 4, 200)\n",
    "            labels = labels.to(device)\n",
    "            #zero the existing gradients so they don't add up\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, labels) \n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        #save training loss \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_error.append(epoch_loss)\n",
    "        \n",
    "        if verbose:\n",
    "            print ('Epoch [{}], Current Train Loss: {:.5f}' \n",
    "                       .format(epoch+1, epoch_loss))\n",
    "            \n",
    "        #save model weights\n",
    "        model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(model_wts, weights_folder + \"/\"+\"model_epoch_\"+str(epoch+1)+\"_\"+\n",
    "                       name_ind+\".pth\") \n",
    "    return model, train_error\n",
    "\n",
    "#function to one-hot encode\n",
    "############################################################\n",
    "def dna_one_hot(seq, seq_len=None, flatten=True):\n",
    "    if seq_len == None:\n",
    "        seq_len = len(seq)\n",
    "        seq_start = 0\n",
    "    else:\n",
    "        if seq_len <= len(seq):\n",
    "            # trim the sequence\n",
    "            seq_trim = (len(seq)-seq_len) // 2\n",
    "            seq = seq[seq_trim:seq_trim+seq_len]\n",
    "            seq_start = 0\n",
    "        else:\n",
    "            seq_start = (seq_len-len(seq)) // 2\n",
    "\n",
    "    seq = seq.upper()\n",
    "\n",
    "    seq = seq.replace('A','0')\n",
    "    seq = seq.replace('C','1')\n",
    "    seq = seq.replace('G','2')\n",
    "    seq = seq.replace('T','3')\n",
    "\n",
    "    # map nt's to a matrix 4 x len(seq) of 0's and 1's.\n",
    "    #  dtype='int8' fails for N's\n",
    "    seq_code = np.zeros((4,seq_len), dtype='float16')\n",
    "    for i in range(seq_len):\n",
    "        if i < seq_start:\n",
    "            seq_code[:,i] = 0.25\n",
    "        else:\n",
    "            try:\n",
    "                seq_code[int(seq[i-seq_start]),i] = 1\n",
    "            except:\n",
    "                seq_code[:,i] = 0.25\n",
    "\n",
    "    # flatten and make a column vector 1 x len(seq)\n",
    "    if flatten:\n",
    "        seq_code = seq_code.flatten()[None,:]\n",
    "\n",
    "    return seq_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ THE FILES\n",
    "pos_seqs = {}\n",
    "neg_seqs = {}\n",
    "\n",
    "fasta_sequences = SeqIO.parse(open(\"./examples/example_pos_sequences.fa\"),'fasta')\n",
    "for fasta in fasta_sequences:\n",
    "    name, sequence = fasta.id, str(fasta.seq).upper()\n",
    "    pos_seqs[name] = sequence\n",
    "    \n",
    "fasta_sequences = SeqIO.parse(open(\"./examples/example_neg_sequences.fa\"),'fasta')\n",
    "for fasta in fasta_sequences:\n",
    "    name, sequence = fasta.id, str(fasta.seq).upper()\n",
    "    neg_seqs[name] = sequence\n",
    "    \n",
    "pos_seqs = pd.Series(pos_seqs)\n",
    "neg_seqs = pd.Series(neg_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONE HOT ENCODE\n",
    "pos_seqs = pos_seqs.map(lambda x: dna_one_hot(x, flatten=False))\n",
    "pos_seqs = np.stack(pos_seqs, axis=0)\n",
    "\n",
    "neg_seqs = neg_seqs.map(lambda x: dna_one_hot(x, flatten=False))\n",
    "neg_seqs = np.stack(neg_seqs, axis=0)\n",
    "\n",
    "all_seq = np.concatenate((pos_seqs, neg_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = np.ones((len(pos_seqs),1))\n",
    "neg_labels = np.zeros((len(pos_seqs),1))\n",
    "\n",
    "all_labels = np.concatenate((pos_labels, neg_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataloader\n",
    "x = torch.Tensor(all_seq)\n",
    "\n",
    "x_lab = torch.Tensor(all_labels)\n",
    "\n",
    "all_dataset = torch.utils.data.TensorDataset(x, x_lab)\n",
    "dataloader = torch.utils.data.DataLoader(all_dataset, \n",
    "                                                  batch_size=100, shuffle=True,\n",
    "                                                  num_workers=0)\n",
    "x_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model\n",
    "class DanQ(nn.Module):\n",
    "    def __init__(self, sequence_length, num_classes, weight_path=None):\n",
    "        super(DanQ, self).__init__()\n",
    "\n",
    "        self._n_channels = math.floor(\n",
    "            (sequence_length - 25) / 13)\n",
    "\n",
    "        self.Conv1 = nn.Conv1d(in_channels=4, out_channels=320, kernel_size=26)\n",
    "        self.Maxpool = nn.MaxPool1d(kernel_size=13, stride=13)\n",
    "        self.Drop1 = nn.Dropout(p=0.2)\n",
    "        self.BiLSTM = nn.LSTM(input_size=320, hidden_size=320, num_layers=2,\n",
    "                                 batch_first=True,\n",
    "                                 dropout=0.5,\n",
    "                                 bidirectional=True)\n",
    "        self.Linear1 = nn.Linear(self._n_channels*640, 925)\n",
    "        self.Linear2 = nn.Linear(925, num_classes)\n",
    "        \n",
    "        if weight_path :\n",
    "            self.load_weights(weight_path)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.Conv1(input)\n",
    "        x = F.relu(x)\n",
    "        x = self.Maxpool(x)\n",
    "        x = self.Drop1(x)\n",
    "        x_x = torch.transpose(x, 1, 2)\n",
    "        x, (h_n,h_c) = self.BiLSTM(x_x)\n",
    "        x = x.contiguous().view(-1, self._n_channels*640)\n",
    "        x = self.Linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.Linear2(x)\n",
    "        return x\n",
    "    \n",
    "    def load_weights(self, weight_path):\n",
    "        sd = torch.load(weight_path)\n",
    "        new_dict = OrderedDict()\n",
    "        keys = list(self.state_dict().keys())\n",
    "        values = list(sd.values())\n",
    "        for i in range(len(values)):\n",
    "            v = values[i]\n",
    "            if v.dim() > 1 :\n",
    "                if v.shape[-1] ==1 :\n",
    "                    new_dict[keys[i]] = v.squeeze(-1)\n",
    "                    continue\n",
    "            new_dict[keys[i]] = v\n",
    "        self.load_state_dict(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DanQ(1000, 1).to(device)\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss() #- no weights\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('weights_DanQ'):\n",
    "    os.makedirs('weights_DanQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-64131db7fd91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model, train_error = train_model(dataloader, model, device, criterion,  \n\u001b[0m\u001b[1;32m      3\u001b[0m                                  \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                  \u001b[0;34m\"weights_DanQ\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                  \"CTCF\", verbose=True)\n",
      "\u001b[0;32m<ipython-input-2-7ded4cd6db8f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, model, device, criterion, optimizer, num_epochs, weights_folder, name_ind, verbose)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/CTCF/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6b0c7ddf4dee>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDrop1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh_c\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBiLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_channels\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/CTCF/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/CTCF/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    577\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "model, train_error = train_model(dataloader, model, device, criterion,  \n",
    "                                 optimizer, num_epochs, \n",
    "                                 \"weights_DanQ\", \n",
    "                                 \"CTCF\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
