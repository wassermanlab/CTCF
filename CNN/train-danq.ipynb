{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gzip\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from torch import relu, sigmoid\n",
    "import torch.nn.modules.activation as activation\n",
    "from Bio import SeqIO\n",
    "\n",
    "from utils.pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DanQ(nn.Module):\n",
    "    \"\"\"DanQ architecture (Quang & Xie, 2016).\"\"\"\n",
    "\n",
    "    def __init__(self, sequence_length, n_targets):\n",
    "        super(DanQ, self).__init__()\n",
    "\n",
    "        self._n_channels = math.floor((sequence_length - 25) / 13)\n",
    "\n",
    "        self.Conv1 = nn.Conv1d(\n",
    "            in_channels=4, out_channels=320, kernel_size=26\n",
    "        )\n",
    "        self.Maxpool = nn.MaxPool1d(kernel_size=13, stride=13)\n",
    "        self.Drop1 = nn.Dropout(p=0.2)\n",
    "        self.BiLSTM = nn.LSTM(\n",
    "            input_size=320, hidden_size=320, num_layers=2,\n",
    "            batch_first=True, dropout=0.5, bidirectional=True\n",
    "        )\n",
    "        self.Linear1 = nn.Linear(self._n_channels*640, 925)\n",
    "        self.Linear2 = nn.Linear(925, n_targets)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.Conv1(input)\n",
    "        x = F.relu(x)\n",
    "        x = self.Maxpool(x)\n",
    "        x = self.Drop1(x)\n",
    "        x_x = torch.transpose(x, 1, 2)\n",
    "        x, (h_n,h_c) = self.BiLSTM(x_x)\n",
    "        x = x.contiguous().view(-1, self._n_channels*640)\n",
    "        x = self.Linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.Linear2(x)\n",
    "\n",
    "        return(x)\n",
    "\n",
    "def get_criterion():\n",
    "    \"\"\"\n",
    "    Specify the appropriate loss function (criterion) for this model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn._Loss\n",
    "    \"\"\"\n",
    "    return(nn.BCEWithLogitsLoss())\n",
    "\n",
    "def get_optimizer(params, lr=0.003):\n",
    "    return(torch.optim.Adam(params, lr=lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(seq):\n",
    "    \"\"\"One hot encodes a sequence.\"\"\"\n",
    "\n",
    "    seq = seq.replace(\"A\", \"0\")\n",
    "    seq = seq.replace(\"C\", \"1\")\n",
    "    seq = seq.replace(\"G\", \"2\")\n",
    "    seq = seq.replace(\"T\", \"3\")\n",
    "\n",
    "    encoded_seq = np.zeros((4, len(seq)), dtype=\"float16\")\n",
    "\n",
    "    for i in range(len(seq)):\n",
    "        if seq[i].isdigit():\n",
    "            encoded_seq[int(seq[i]), i] = 1\n",
    "        else:\n",
    "            # i.e. Ns\n",
    "            encoded_seq[:, i] = 0.25\n",
    "\n",
    "    return(encoded_seq)\n",
    "\n",
    "def one_hot_decode(encoded_seq):\n",
    "    \"\"\"Reverts a sequence's one hot encoding.\"\"\"\n",
    "\n",
    "    seq = []\n",
    "    code = list(\"ACGT\")\n",
    " \n",
    "    for i in encoded_seq.transpose(1, 0):\n",
    "        try:\n",
    "            seq.append(code[int(np.where(i == 1)[0])])\n",
    "        except:\n",
    "            # i.e. N?\n",
    "            seq.append(\"N\")\n",
    "\n",
    "    return(\"\".join(seq))\n",
    "\n",
    "def reverse_complement(encoded_seqs):\n",
    "    \"\"\"Reverse complements one hot encoding for a list of sequences.\"\"\"\n",
    "    return(encoded_seqs[..., ::-1, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7          GGACAGGTCAACTTGAGGAGATTTTGGGCCTTCATAGGCCACCAGG...\n",
       "16         CCACATTATACAGCTTCTGAAAGGGTTGCTTGACCCACAGATGTGA...\n",
       "22         GAAGGAGACTGATGTGGTTTCTCCTCAGTTTCTCTGTGCGGCACCA...\n",
       "49         ACCTCTATGGTGTCGGCGAAGACCCGCCCTTGTGACGTCACGGAAG...\n",
       "107        GGGAATGCTAAACAGAGGCAGATCTAAACTTAGGAGTTAGGCTTCT...\n",
       "                                 ...                        \n",
       "1817711    TGCTAGGAGCCGCAGTCATACTGGCTGTGCATGAGACCATCCACCT...\n",
       "1817721    AAGGCAAAGTGAGAAAAAGAGGAAACTAGAAGGCTGGTTGGGCTGT...\n",
       "1817723    CCTTGTCTTGGCATTTTCGGAGAGAACATGGACTCTGTGTTGTTTG...\n",
       "1817732    CTCTTACTCTTTCTGTGTGTGAAATGTGCAAGTAGCTTTACAGTCT...\n",
       "1817832    TCTTCTTTCCCTTTCCCTCCTCCCTAGGGGGTGTGACTGTAGAGCA...\n",
       "Length: 78983, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse FASTA sequences\n",
    "pos_seqs = {}\n",
    "neg_seqs = {}\n",
    "with gzip.open(\"../Data/pos_seqs.fa.gz\", \"rt\") as handle:\n",
    "    for seq_record in SeqIO.parse(handle, \"fasta\"):\n",
    "        pos_seqs[seq_record.id] = str(seq_record.seq).upper()\n",
    "pos_seqs = pd.Series(pos_seqs)\n",
    "with gzip.open(\"../Data/neg_seqs.fa.gz\", \"rt\") as handle:\n",
    "    for seq_record in SeqIO.parse(handle, \"fasta\"):\n",
    "        neg_seqs[seq_record.id] = str(seq_record.seq).upper()\n",
    "neg_seqs = pd.Series(neg_seqs)\n",
    "pos_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 1., 1., ..., 1., 1., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 1., 1., 1.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 1., 0., ..., 1., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 1., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 1., ..., 0., 0., 1.]]], dtype=float16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode sequences\n",
    "pos_seqs_1_hot = pos_seqs.map(lambda x: one_hot_encode(x))\n",
    "pos_seqs_1_hot = np.stack(pos_seqs_1_hot, axis=0)\n",
    "neg_seqs_1_hot = neg_seqs.map(lambda x: one_hot_encode(x))\n",
    "neg_seqs_1_hot = np.stack(neg_seqs_1_hot, axis=0)\n",
    "pos_seqs_1_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 1., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 1., ..., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 1., 1.],\n",
       "        [1., 0., 1., ..., 1., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 1., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 1., 1.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 1., 1., ..., 0., 0., 1.],\n",
       "        [1., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[1., 0., 1., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]]], dtype=float16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split sequences into train, validation and test\n",
    "seed = 123\n",
    "pos_train_seqs, pos_test_seqs = train_test_split(\n",
    "    pos_seqs_1_hot, test_size=0.2, random_state=seed\n",
    ")\n",
    "pos_validation_seqs, pos_test_seqs = train_test_split(\n",
    "    pos_test_seqs, test_size=0.5, random_state=seed\n",
    ")\n",
    "neg_train_seqs, neg_test_seqs = train_test_split(\n",
    "    neg_seqs_1_hot, test_size=0.2, random_state=seed\n",
    ")\n",
    "neg_validation_seqs, neg_test_seqs = train_test_split(\n",
    "    neg_test_seqs, test_size=0.5, random_state=seed\n",
    ")\n",
    "pos_train_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 1., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 1., ..., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 1., 1.],\n",
       "        [1., 0., 1., ..., 1., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 1., 0., ..., 0., 1., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 0., 1.],\n",
       "        [1., 0., 0., ..., 1., 1., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 1., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 1., 0., 1.]]], dtype=float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reverse complement train\n",
    "pos_train_seqs_rc = np.append(pos_train_seqs, reverse_complement(pos_train_seqs), axis=0)\n",
    "neg_train_seqs_rc = np.append(neg_train_seqs, reverse_complement(pos_train_seqs), axis=0)\n",
    "pos_train_seqs_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7fac2530ca00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a TensorDatasets\n",
    "X = np.concatenate((pos_train_seqs_rc, neg_train_seqs_rc))\n",
    "y = np.concatenate(\n",
    "    (np.ones((len(pos_train_seqs_rc), 1)), np.zeros((len(neg_train_seqs_rc), 1)))\n",
    ")\n",
    "train_dataset = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "X = np.concatenate((pos_validation_seqs, neg_validation_seqs))\n",
    "y = np.concatenate(\n",
    "    (np.ones((len(pos_validation_seqs), 1)), np.zeros((len(neg_validation_seqs), 1)))\n",
    ")\n",
    "validation_dataset = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "X = np.concatenate((pos_test_seqs, neg_test_seqs))\n",
    "y = np.concatenate(\n",
    "    (np.ones((len(pos_test_seqs), 1)), np.zeros((len(neg_test_seqs), 1)))\n",
    ")\n",
    "test_dataset = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fac22f9e100>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "parameters = dict(batch_size=64, shuffle=True, num_workers=8)\n",
    "train_dataloader = DataLoader(train_dataset, **parameters)\n",
    "validation_dataloader = DataLoader(validation_dataset, **parameters)\n",
    "test_dataloader = DataLoader(test_dataset, **parameters)\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/100] lr: 0.003 train_loss: 0.69588 (81.969 sec) valid_loss: 0.69316 (2.451 sec)\n",
      "Validation loss decreased (inf --> 0.693156), saving model ...\n",
      "[  2/100] lr: 0.003 train_loss: 0.69318 (81.489 sec) valid_loss: 0.69319 (2.295 sec)\n",
      "EarlyStopping counter: 1 out of 10\n",
      "[  3/100] lr: 0.003 train_loss: 0.69318 (82.881 sec) valid_loss: 0.69317 (2.377 sec)\n",
      "EarlyStopping counter: 2 out of 10\n",
      "[  4/100] lr: 0.003 train_loss: 0.69318 (80.840 sec) valid_loss: 0.69333 (2.155 sec)\n",
      "EarlyStopping counter: 3 out of 10\n",
      "[  5/100] lr: 0.003 train_loss: 0.69321 (81.982 sec) valid_loss: 0.69315 (2.277 sec)\n",
      "Validation loss decreased (0.693156 --> 0.693147), saving model ...\n",
      "[  6/100] lr: 0.003 train_loss: 0.69320 (71.562 sec) valid_loss: 0.69316 (2.252 sec)\n",
      "EarlyStopping counter: 1 out of 10\n",
      "[  7/100] lr: 0.003 train_loss: 0.69319 (83.176 sec) valid_loss: 0.69315 (2.410 sec)\n",
      "EarlyStopping counter: 2 out of 10\n",
      "[  8/100] lr: 0.003 train_loss: 0.69319 (82.780 sec) valid_loss: 0.69336 (2.338 sec)\n",
      "EarlyStopping counter: 3 out of 10\n",
      "[  9/100] lr: 0.003 train_loss: 0.69320 (83.391 sec) valid_loss: 0.69326 (2.305 sec)\n",
      "EarlyStopping counter: 4 out of 10\n",
      "[ 10/100] lr: 0.003 train_loss: 0.69320 (81.075 sec) valid_loss: 0.69325 (2.348 sec)\n",
      "EarlyStopping counter: 5 out of 10\n",
      "[ 11/100] lr: 0.003 train_loss: 0.69320 (82.814 sec) valid_loss: 0.69316 (2.323 sec)\n",
      "EarlyStopping counter: 6 out of 10\n",
      "[ 12/100] lr: 0.003 train_loss: 0.69320 (72.713 sec) valid_loss: 0.69316 (2.429 sec)\n",
      "EarlyStopping counter: 7 out of 10\n",
      "[ 13/100] lr: 0.003 train_loss: 0.69319 (83.000 sec) valid_loss: 0.69319 (2.371 sec)\n",
      "EarlyStopping counter: 8 out of 10\n",
      "[ 14/100] lr: 0.003 train_loss: 0.69319 (81.994 sec) valid_loss: 0.69315 (2.380 sec)\n",
      "EarlyStopping counter: 9 out of 10\n",
      "[ 15/100] lr: 0.003 train_loss: 0.69320 (82.314 sec) valid_loss: 0.69316 (2.443 sec)\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Stop!!!\n",
      "[  1/100] lr: 0.001 train_loss: 0.69340 (82.373 sec) valid_loss: 0.69315 (2.339 sec)\n",
      "Validation loss decreased (inf --> 0.693147), saving model ...\n",
      "[  2/100] lr: 0.001 train_loss: 0.69317 (79.958 sec) valid_loss: 0.69315 (1.508 sec)\n",
      "EarlyStopping counter: 1 out of 10\n",
      "[  3/100] lr: 0.001 train_loss: 0.69317 (75.586 sec) valid_loss: 0.69315 (2.367 sec)\n",
      "Validation loss decreased (0.693147 --> 0.693147), saving model ...\n",
      "[  4/100] lr: 0.001 train_loss: 0.69316 (82.681 sec) valid_loss: 0.69315 (2.393 sec)\n",
      "EarlyStopping counter: 1 out of 10\n",
      "[  5/100] lr: 0.001 train_loss: 0.69316 (82.245 sec) valid_loss: 0.69315 (2.284 sec)\n",
      "EarlyStopping counter: 2 out of 10\n",
      "[  6/100] lr: 0.001 train_loss: 0.69316 (82.205 sec) valid_loss: 0.69319 (2.343 sec)\n",
      "EarlyStopping counter: 3 out of 10\n",
      "[  7/100] lr: 0.001 train_loss: 0.69316 (81.963 sec) valid_loss: 0.69315 (2.359 sec)\n",
      "EarlyStopping counter: 4 out of 10\n",
      "[  8/100] lr: 0.001 train_loss: 0.69317 (72.470 sec) valid_loss: 0.69315 (1.440 sec)\n",
      "EarlyStopping counter: 5 out of 10\n",
      "[  9/100] lr: 0.001 train_loss: 0.69316 (80.588 sec) valid_loss: 0.69320 (2.330 sec)\n",
      "EarlyStopping counter: 6 out of 10\n",
      "[ 10/100] lr: 0.001 train_loss: 0.69316 (82.682 sec) valid_loss: 0.69318 (2.215 sec)\n",
      "EarlyStopping counter: 7 out of 10\n",
      "[ 11/100] lr: 0.001 train_loss: 0.69317 (81.486 sec) valid_loss: 0.69316 (2.240 sec)\n",
      "EarlyStopping counter: 8 out of 10\n",
      "[ 12/100] lr: 0.001 train_loss: 0.69316 (82.202 sec) valid_loss: 0.69319 (2.203 sec)\n",
      "EarlyStopping counter: 9 out of 10\n",
      "[ 13/100] lr: 0.001 train_loss: 0.69316 (83.446 sec) valid_loss: 0.69317 (2.410 sec)\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Stop!!!\n",
      "[  1/100] lr: 0.0003 train_loss: 0.63042 (75.028 sec) valid_loss: 0.43039 (2.348 sec)\n",
      "Validation loss decreased (inf --> 0.430388), saving model ...\n",
      "[  2/100] lr: 0.0003 train_loss: 0.58166 (83.018 sec) valid_loss: 0.37649 (2.303 sec)\n",
      "Validation loss decreased (0.430388 --> 0.376495), saving model ...\n",
      "[  3/100] lr: 0.0003 train_loss: 0.57429 (82.780 sec) valid_loss: 0.37892 (2.325 sec)\n",
      "EarlyStopping counter: 1 out of 10\n",
      "[  4/100] lr: 0.0003 train_loss: 0.56951 (83.759 sec) valid_loss: 0.39083 (2.341 sec)\n",
      "EarlyStopping counter: 2 out of 10\n",
      "[  5/100] lr: 0.0003 train_loss: 0.56573 (83.795 sec) valid_loss: 0.38505 (2.244 sec)\n",
      "EarlyStopping counter: 3 out of 10\n",
      "[  6/100] lr: 0.0003 train_loss: 0.56278 (83.174 sec) valid_loss: 0.37277 (2.315 sec)\n",
      "Validation loss decreased (0.376495 --> 0.372767), saving model ...\n",
      "[  7/100] lr: 0.0003 train_loss: 0.55988 (74.285 sec) valid_loss: 0.38535 (2.339 sec)\n",
      "EarlyStopping counter: 1 out of 10\n",
      "[  8/100] lr: 0.0003 train_loss: 0.55717 (84.834 sec) valid_loss: 0.38166 (2.447 sec)\n",
      "EarlyStopping counter: 2 out of 10\n",
      "[  9/100] lr: 0.0003 train_loss: 0.55433 (83.875 sec) valid_loss: 0.36123 (2.376 sec)\n",
      "Validation loss decreased (0.372767 --> 0.361228), saving model ...\n",
      "[ 10/100] lr: 0.0003 train_loss: 0.55194 (83.169 sec) valid_loss: 0.37483 (2.384 sec)\n",
      "EarlyStopping counter: 1 out of 10\n",
      "[ 11/100] lr: 0.0003 train_loss: 0.54988 (83.938 sec) valid_loss: 0.38515 (2.350 sec)\n",
      "EarlyStopping counter: 2 out of 10\n",
      "[ 12/100] lr: 0.0003 train_loss: 0.54743 (79.989 sec) valid_loss: 0.38172 (1.638 sec)\n",
      "EarlyStopping counter: 3 out of 10\n",
      "[ 13/100] lr: 0.0003 train_loss: 0.54565 (78.432 sec) valid_loss: 0.36802 (2.343 sec)\n",
      "EarlyStopping counter: 4 out of 10\n",
      "[ 14/100] lr: 0.0003 train_loss: 0.54368 (84.256 sec) valid_loss: 0.38902 (2.506 sec)\n",
      "EarlyStopping counter: 5 out of 10\n",
      "[ 15/100] lr: 0.0003 train_loss: 0.54270 (83.127 sec) valid_loss: 0.39369 (2.283 sec)\n",
      "EarlyStopping counter: 6 out of 10\n",
      "[ 16/100] lr: 0.0003 train_loss: 0.54084 (83.258 sec) valid_loss: 0.38413 (2.372 sec)\n",
      "EarlyStopping counter: 7 out of 10\n",
      "[ 17/100] lr: 0.0003 train_loss: 0.53963 (83.808 sec) valid_loss: 0.38887 (2.314 sec)\n",
      "EarlyStopping counter: 8 out of 10\n",
      "[ 18/100] lr: 0.0003 train_loss: 0.53735 (73.718 sec) valid_loss: 0.39486 (2.386 sec)\n",
      "EarlyStopping counter: 9 out of 10\n",
      "[ 19/100] lr: 0.0003 train_loss: 0.53695 (83.487 sec) valid_loss: 0.39086 (2.607 sec)\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Stop!!!\n",
      "[  1/100] lr: 0.0001 train_loss: 0.66751 (84.905 sec) valid_loss: 0.50229 (2.474 sec)\n",
      "Validation loss decreased (inf --> 0.502286), saving model ...\n",
      "[  2/100] lr: 0.0001 train_loss: 0.61177 (81.881 sec) valid_loss: 0.44097 (2.228 sec)\n",
      "Validation loss decreased (0.502286 --> 0.440969), saving model ...\n",
      "[  3/100] lr: 0.0001 train_loss: 0.59267 (83.753 sec) valid_loss: 0.40650 (2.398 sec)\n",
      "Validation loss decreased (0.440969 --> 0.406504), saving model ...\n",
      "[  4/100] lr: 0.0001 train_loss: 0.58242 (83.095 sec) valid_loss: 0.38913 (2.291 sec)\n",
      "Validation loss decreased (0.406504 --> 0.389130), saving model ...\n",
      "[  5/100] lr: 0.0001 train_loss: 0.57569 (72.423 sec) valid_loss: 0.38635 (2.187 sec)\n",
      "Validation loss decreased (0.389130 --> 0.386350), saving model ...\n",
      "[  6/100] lr: 0.0001 train_loss: 0.57114 (82.714 sec) valid_loss: 0.38330 (2.437 sec)\n",
      "Validation loss decreased (0.386350 --> 0.383303), saving model ...\n",
      "[  7/100] lr: 0.0001 train_loss: 0.56770 (82.960 sec) valid_loss: 0.37399 (2.221 sec)\n",
      "Validation loss decreased (0.383303 --> 0.373990), saving model ...\n",
      "[  8/100] lr: 0.0001 train_loss: 0.56508 (82.800 sec) valid_loss: 0.36703 (2.329 sec)\n",
      "Validation loss decreased (0.373990 --> 0.367035), saving model ...\n",
      "[  9/100] lr: 0.0001 train_loss: 0.56205 (84.440 sec) valid_loss: 0.38680 (2.465 sec)\n",
      "EarlyStopping counter: 1 out of 10\n",
      "[ 10/100] lr: 0.0001 train_loss: 0.55949 (84.853 sec) valid_loss: 0.38237 (1.699 sec)\n",
      "EarlyStopping counter: 2 out of 10\n",
      "[ 11/100] lr: 0.0001 train_loss: 0.55727 (73.299 sec) valid_loss: 0.38316 (2.466 sec)\n",
      "EarlyStopping counter: 3 out of 10\n",
      "[ 12/100] lr: 0.0001 train_loss: 0.55484 (81.625 sec) valid_loss: 0.38910 (2.309 sec)\n",
      "EarlyStopping counter: 4 out of 10\n",
      "[ 13/100] lr: 0.0001 train_loss: 0.55346 (82.273 sec) valid_loss: 0.38117 (2.262 sec)\n",
      "EarlyStopping counter: 5 out of 10\n",
      "[ 14/100] lr: 0.0001 train_loss: 0.55154 (84.612 sec) valid_loss: 0.37741 (2.185 sec)\n",
      "EarlyStopping counter: 6 out of 10\n",
      "[ 15/100] lr: 0.0001 train_loss: 0.54960 (83.233 sec) valid_loss: 0.37893 (2.303 sec)\n",
      "EarlyStopping counter: 7 out of 10\n",
      "[ 16/100] lr: 0.0001 train_loss: 0.54789 (77.858 sec) valid_loss: 0.38222 (1.680 sec)\n",
      "EarlyStopping counter: 8 out of 10\n",
      "[ 17/100] lr: 0.0001 train_loss: 0.54631 (79.309 sec) valid_loss: 0.38940 (2.418 sec)\n",
      "EarlyStopping counter: 9 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 18/100] lr: 0.0001 train_loss: 0.54522 (83.577 sec) valid_loss: 0.37832 (2.308 sec)\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Stop!!!\n"
     ]
    }
   ],
   "source": [
    "# Train and validate\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_epochs = 100\n",
    "learning_rates = [0.003, 0.001, 0.0003, 0.0001]\n",
    "output_dir = \"./CTCF/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for lr in learning_rates:\n",
    "\n",
    "    # Initialize model, criterion, optimizer\n",
    "    model = DanQ(len(pos_seqs[0]), 1).to(device)\n",
    "    criterion = get_criterion()\n",
    "    optimizer = get_optimizer(model.parameters(), lr)\n",
    "    state_dict = os.path.join(output_dir, \"model-%s.pth.tar\" % lr)\n",
    "    early_stopping = EarlyStopping(10, True, path=state_dict)\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "\n",
    "        # Train\n",
    "        t_time = time()\n",
    "        model.train() # set the model in train mode\n",
    "        train_losses.append([])\n",
    "        for seqs, labels in train_dataloader:\n",
    "            x = seqs.to(device) # shape = (batch_size, 4, 200)\n",
    "            labels = labels.to(device)\n",
    "            # Zero existing gradients so they don't add up\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, labels) \n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Keep the loss\n",
    "            train_losses[-1].append(loss.item())\n",
    "        t_loss = np.average(train_losses[-1])\n",
    "        t_time = time() - t_time\n",
    "\n",
    "        # Validate\n",
    "        v_time = time()\n",
    "        model.eval() # set the model in evaluation mode\n",
    "        validation_losses.append([])\n",
    "        for seqs, labels in validation_dataloader:\n",
    "            x = seqs.to(device) # shape = (batch_size, 4, 200)\n",
    "            labels = labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                # Forward pass\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, labels) \n",
    "                # Keep the loss\n",
    "                validation_losses[-1].append(loss.item())\n",
    "        v_loss = np.average(validation_losses[-1])\n",
    "        v_time = time() - v_time\n",
    "\n",
    "        print(f'[{epoch:>{3}}/{max_epochs:>{3}}] '\n",
    "             +f'lr: {lr} '\n",
    "             +f'train_loss: {t_loss:.5f} ({t_time:.3f} sec) '\n",
    "             +f'valid_loss: {v_loss:.5f} ({v_time:.3f} sec)')\n",
    "\n",
    "        # EarlyStopping needs to check if the validation loss has decresed, \n",
    "        # and if it has, it will save the current model.\n",
    "        early_stopping(v_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            # Empty cache\n",
    "            with torch.cuda.device(device):\n",
    "                torch.cuda.empty_cache()\n",
    "            print(\"Stop!!!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997],\n",
      "        [0.4997]], device='cuda:0')\n",
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], device='cuda:0')\n",
      "tensor([[0.6916],\n",
      "        [0.2842],\n",
      "        [0.6768],\n",
      "        [0.0337],\n",
      "        [0.5848],\n",
      "        [0.0342],\n",
      "        [0.6566],\n",
      "        [0.6447],\n",
      "        [0.6753],\n",
      "        [0.0013],\n",
      "        [0.0791],\n",
      "        [0.6818],\n",
      "        [0.6895],\n",
      "        [0.5974],\n",
      "        [0.6709],\n",
      "        [0.1129],\n",
      "        [0.5888],\n",
      "        [0.7185],\n",
      "        [0.1756],\n",
      "        [0.6847],\n",
      "        [0.0914],\n",
      "        [0.0284],\n",
      "        [0.3157],\n",
      "        [0.6865],\n",
      "        [0.0031],\n",
      "        [0.6810],\n",
      "        [0.6684],\n",
      "        [0.6934],\n",
      "        [0.0429],\n",
      "        [0.6991],\n",
      "        [0.4987],\n",
      "        [0.1510],\n",
      "        [0.0125],\n",
      "        [0.0153],\n",
      "        [0.6845],\n",
      "        [0.4236],\n",
      "        [0.4535],\n",
      "        [0.3134],\n",
      "        [0.6836],\n",
      "        [0.5331],\n",
      "        [0.6108],\n",
      "        [0.6831],\n",
      "        [0.6799],\n",
      "        [0.6668],\n",
      "        [0.0161],\n",
      "        [0.6812],\n",
      "        [0.6755],\n",
      "        [0.0181],\n",
      "        [0.0696],\n",
      "        [0.0550],\n",
      "        [0.0834],\n",
      "        [0.0565],\n",
      "        [0.6665],\n",
      "        [0.0313],\n",
      "        [0.7214],\n",
      "        [0.0110],\n",
      "        [0.0101],\n",
      "        [0.7024],\n",
      "        [0.0260],\n",
      "        [0.6829],\n",
      "        [0.3003],\n",
      "        [0.6640],\n",
      "        [0.0237],\n",
      "        [0.6470]], device='cuda:0')\n",
      "tensor([[0.1316],\n",
      "        [0.5398],\n",
      "        [0.6901],\n",
      "        [0.7096],\n",
      "        [0.0453],\n",
      "        [0.1766],\n",
      "        [0.7095],\n",
      "        [0.6404],\n",
      "        [0.0319],\n",
      "        [0.6582],\n",
      "        [0.0358],\n",
      "        [0.6560],\n",
      "        [0.3174],\n",
      "        [0.3992],\n",
      "        [0.6152],\n",
      "        [0.0439],\n",
      "        [0.6837],\n",
      "        [0.0423],\n",
      "        [0.0618],\n",
      "        [0.6596],\n",
      "        [0.0528],\n",
      "        [0.5845],\n",
      "        [0.1928],\n",
      "        [0.6668],\n",
      "        [0.6305],\n",
      "        [0.6683],\n",
      "        [0.0834],\n",
      "        [0.1613],\n",
      "        [0.1927],\n",
      "        [0.1032],\n",
      "        [0.6665],\n",
      "        [0.6439],\n",
      "        [0.6535],\n",
      "        [0.1002],\n",
      "        [0.6987],\n",
      "        [0.6216],\n",
      "        [0.6589],\n",
      "        [0.6907],\n",
      "        [0.6953],\n",
      "        [0.6958],\n",
      "        [0.7126],\n",
      "        [0.6865],\n",
      "        [0.6813],\n",
      "        [0.6675],\n",
      "        [0.0469],\n",
      "        [0.4214],\n",
      "        [0.6446],\n",
      "        [0.6749],\n",
      "        [0.5341],\n",
      "        [0.1117],\n",
      "        [0.5891],\n",
      "        [0.7135],\n",
      "        [0.6388],\n",
      "        [0.1089],\n",
      "        [0.6058],\n",
      "        [0.0904],\n",
      "        [0.6937],\n",
      "        [0.6280],\n",
      "        [0.6666],\n",
      "        [0.0765],\n",
      "        [0.6871],\n",
      "        [0.6808],\n",
      "        [0.2703],\n",
      "        [0.0599]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Compute performance metrics\n",
    "def compute_performance_metrics(predictions, labels, lr):\n",
    "\n",
    "    # Metrics\n",
    "    metrics = dict(AUCPR=None, AUCROC=None, MCC=None)\n",
    "\n",
    "    # Metrics to DataFrame\n",
    "    for metric in metrics:\n",
    "        if metric == \"AUCPR\":\n",
    "            score = average_precision_score(labels, predictions)\n",
    "            self.metrics.setdefault(metric, score)\n",
    "            prec, recall, _ = precision_recall_curve(labels, predictions)\n",
    "            # i.e. precision = 0, recall = 1\n",
    "            prec = np.insert(prec, 0, 0., axis=0)\n",
    "            recall = np.insert(recall, 0, 1., axis=0)\n",
    "            data = list(zip(recall, prec))\n",
    "            self.__visualize_metric(data, [\"Recall\", \"Precision\"], metric)\n",
    "        elif metric == \"AUCROC\":\n",
    "            score = roc_auc_score(labels, predictions)\n",
    "            self.metrics.setdefault(metric, score)\n",
    "            fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "            data = list(zip(fpr, tpr))\n",
    "            self.__visualize_metric(data, [\"Fpr\", \"Tpr\"], metric)\n",
    "        elif metric == \"MCC\":\n",
    "            score = matthews_corrcoef(labels, np.rint(predictions))\n",
    "            self.metrics.setdefault(metric, score)\n",
    "    \n",
    "    if self._verbose:\n",
    "        write(\n",
    "            None,\n",
    "            (f'Final performance metrics: '\n",
    "            +f'AUCROC: {self.metrics[\"AUCROC\"]:.5f}, '\n",
    "            +f'AUCPR: {self.metrics[\"AUCPR\"]:.5f}, '\n",
    "            +f'MCC: {self.metrics[\"MCC\"]:.5f}')\n",
    "        )\n",
    "\n",
    "def __visualize_metric(self, data, labels, metric):\n",
    "\n",
    "    # Metric to DataFrame\n",
    "    df = pd.DataFrame(data, columns=labels)\n",
    "\n",
    "    # Seaborn aesthetics\n",
    "    sns.set_context(\"paper\", font_scale=1.5, rc={\"lines.linewidth\": 1.5})\n",
    "    sns.set_palette(sns.color_palette([\"#1965B0\"]))\n",
    "\n",
    "    # Plot metric\n",
    "    kwargs = dict(estimator=None, ci=None)\n",
    "    g = sns.lineplot(x=labels[0], y=labels[1], data=df, **kwargs)\n",
    "\n",
    "    # Add metric score\n",
    "    kwargs = dict(horizontalalignment=\"center\", verticalalignment=\"center\")\n",
    "    plt.text(.5, 0, \"%s = %.5f\" % (metric, self.metrics[metric]), **kwargs)\n",
    "\n",
    "    # Remove spines\n",
    "    sns.despine()\n",
    "\n",
    "    # Save & close\n",
    "    fig = g.get_figure()\n",
    "    fig.savefig(os.path.join(self.output_dir, \"%s.png\" % metric))\n",
    "    plt.close(fig)\n",
    "\n",
    "for lr in learning_rates:\n",
    "   \n",
    "    # Load the best model\n",
    "    labels = None\n",
    "    predictions = None\n",
    "    model = DanQ(len(pos_seqs[0]), 1).to(device)\n",
    "    state_dict = os.path.join(output_dir, \"model-%s.pth.tar\" % lr)\n",
    "    model.load_state_dict(torch.load(state_dict))\n",
    "    model.eval() # set the model in evaluation mode\n",
    "\n",
    "    for inputs, targets in test_dataloader:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = torch.sigmoid(model(inputs))\n",
    "\n",
    "            if predictions is None and labels is None:\n",
    "                predictions = outputs.data.cpu().numpy()\n",
    "                labels = targets.data.cpu().numpy()\n",
    "            else:\n",
    "                predictions = np.append(\n",
    "                    predictions, outputs.data.cpu().numpy(), axis=0\n",
    "                )\n",
    "                labels = np.append(\n",
    "                    labels, targets.data.cpu().numpy(), axis=0\n",
    "                )\n",
    "\n",
    "    compute_performance_metrics(predictions.flatten(), labels.flatten(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "         1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
       "         1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "         1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
       "         1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "         1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
       "         0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
       "         0., 1., 1., 0., 0., 0., 1., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "         0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 1., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float16)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fwd = pos_train[:1]\n",
    "fwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GCATCCACACACCCTCAGATGCTTCCTTTGACGCCCTCTGCTGTGCCCCTAGACACCCCTATCCCGCCACTGGCTGAAGCTGGACTTTGGAGCCATCTGCCTCCCTTGCCTGCGTCCACACCCCGCGCCAGTCCTCAGCCTCCAAGCCCATCTCAGTCGGACCCTTTCTCATTCCTGCCACTCGCTGCCTGTTCCAGGCC'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_decode(fwd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
       "         0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "         1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
       "         0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
       "         1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "         0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "         1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
       "         1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "         1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "         1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
       "         1., 0., 1., 1., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "         0., 1., 0., 0., 0., 1., 0., 0.]]], dtype=float16)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev = reverse_complement_one_hot_encoding(fwd)\n",
    "rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GGCCTGGAACAGGCAGCGAGTGGCAGGAATGAGAAAGGGTCCGACTGAGATGGGCTTGGAGGCTGAGGACTGGCGCGGGGTGTGGACGCAGGCAAGGGAGGCAGATGGCTCCAAAGTCCAGCTTCAGCCAGTGGCGGGATAGGGGTGTCTAGGGGCACAGCAGAGGGCGTCAAAGGAAGCATCTGAGGGTGTGTGGATGC'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_decode(rev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to train a model - no validation\n",
    "def train_model(train_loader, model, device, criterion, optimizer, num_epochs,\n",
    "               weights_folder, name_ind, verbose):\n",
    "    total_step = len(train_loader)\n",
    "    train_error = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() #tell model explicitly that we train\n",
    "        logs = {}\n",
    "        running_loss = 0.0\n",
    "        for seqs, labels in train_loader:\n",
    "            x = seqs.to(device) #the input here is (batch_size, 4, 200)\n",
    "            labels = labels.to(device)\n",
    "            #zero the existing gradients so they don't add up\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, labels) \n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        #save training loss \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_error.append(epoch_loss)\n",
    "        \n",
    "        if verbose:\n",
    "            print ('Epoch [{}], Current Train Loss: {:.5f}' \n",
    "                       .format(epoch+1, epoch_loss))\n",
    "            \n",
    "        #save model weights\n",
    "        model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(model_wts, weights_folder + \"/\"+\"model_epoch_\"+str(epoch+1)+\"_\"+\n",
    "                       name_ind+\".pth\") \n",
    "    return model, train_error\n",
    "\n",
    "#function to one-hot encode\n",
    "############################################################\n",
    "def dna_one_hot(seq, seq_len=None, flatten=True):\n",
    "    if seq_len == None:\n",
    "        seq_len = len(seq)\n",
    "        seq_start = 0\n",
    "    else:\n",
    "        if seq_len <= len(seq):\n",
    "            # trim the sequence\n",
    "            seq_trim = (len(seq)-seq_len) // 2\n",
    "            seq = seq[seq_trim:seq_trim+seq_len]\n",
    "            seq_start = 0\n",
    "        else:\n",
    "            seq_start = (seq_len-len(seq)) // 2\n",
    "\n",
    "    seq = seq.upper()\n",
    "\n",
    "    seq = seq.replace('A','0')\n",
    "    seq = seq.replace('C','1')\n",
    "    seq = seq.replace('G','2')\n",
    "    seq = seq.replace('T','3')\n",
    "\n",
    "    # map nt's to a matrix 4 x len(seq) of 0's and 1's.\n",
    "    #  dtype='int8' fails for N's\n",
    "    seq_code = np.zeros((4,seq_len), dtype='float16')\n",
    "    for i in range(seq_len):\n",
    "        if i < seq_start:\n",
    "            seq_code[:,i] = 0.25\n",
    "        else:\n",
    "            try:\n",
    "                seq_code[int(seq[i-seq_start]),i] = 1\n",
    "            except:\n",
    "                seq_code[:,i] = 0.25\n",
    "\n",
    "    # flatten and make a column vector 1 x len(seq)\n",
    "    if flatten:\n",
    "        seq_code = seq_code.flatten()[None,:]\n",
    "\n",
    "    return seq_code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
