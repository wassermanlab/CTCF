{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import gzip\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, precision_recall_curve,\n",
    "    roc_auc_score, roc_curve,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from utils.pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://github.com/FunctionLab/selene/blob/master/models/danQ.py\n",
    "class DanQ(nn.Module):\n",
    "    \"\"\"DanQ architecture (Quang & Xie, 2016).\"\"\"\n",
    "\n",
    "    def __init__(self, sequence_length, n_features):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        sequence_length : int\n",
    "            Input sequence length\n",
    "        n_features : int\n",
    "            Total number of features to predict\n",
    "        \"\"\"\n",
    "        super(DanQ, self).__init__()\n",
    "\n",
    "        self.nnet = nn.Sequential(\n",
    "            nn.Conv1d(4, 320, kernel_size=26),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=13, stride=13),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.bdlstm = nn.Sequential(\n",
    "            nn.LSTM(320, 320, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        )\n",
    "\n",
    "        self._n_channels = math.floor((sequence_length - 25) / 13)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self._n_channels * 640, 925),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(925, n_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward propagation of a batch.\"\"\"\n",
    "        out = self.nnet(x)\n",
    "        reshape_out = out.transpose(0, 1).transpose(0, 2)\n",
    "        out, _ = self.bdlstm(reshape_out)\n",
    "        out = out.transpose(0, 1)\n",
    "        reshape_out = out.contiguous().view(\n",
    "            out.size(0), 640 * self._n_channels)\n",
    "        predict = self.classifier(reshape_out)\n",
    "\n",
    "        return(predict)\n",
    "\n",
    "def get_criterion():\n",
    "    \"\"\"\n",
    "    Specify the appropriate loss function (criterion) for this model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn._Loss\n",
    "    \"\"\"\n",
    "    return(nn.BCELoss())\n",
    "\n",
    "def get_optimizer(params, lr=0.001):\n",
    "    return(torch.optim.Adam(params, lr=lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(seq):\n",
    "    \"\"\"One hot encodes a sequence.\"\"\"\n",
    "\n",
    "    seq = seq.replace(\"A\", \"0\")\n",
    "    seq = seq.replace(\"C\", \"1\")\n",
    "    seq = seq.replace(\"G\", \"2\")\n",
    "    seq = seq.replace(\"T\", \"3\")\n",
    "\n",
    "    encoded_seq = np.zeros((4, len(seq)), dtype=\"float16\")\n",
    "\n",
    "    for i in range(len(seq)):\n",
    "        if seq[i].isdigit():\n",
    "            encoded_seq[int(seq[i]), i] = 1\n",
    "        else:\n",
    "            # i.e. Ns\n",
    "            encoded_seq[:, i] = 0.25\n",
    "\n",
    "    return(encoded_seq)\n",
    "\n",
    "def one_hot_decode(encoded_seq):\n",
    "    \"\"\"Reverts a sequence's one hot encoding.\"\"\"\n",
    "\n",
    "    seq = []\n",
    "    code = list(\"ACGT\")\n",
    " \n",
    "    for i in encoded_seq.transpose(1, 0):\n",
    "        try:\n",
    "            seq.append(code[int(np.where(i == 1)[0])])\n",
    "        except:\n",
    "            # i.e. N?\n",
    "            seq.append(\"N\")\n",
    "\n",
    "    return(\"\".join(seq))\n",
    "\n",
    "def reverse_complement(encoded_seqs):\n",
    "    \"\"\"Reverse complements one hot encoding for a list of sequences.\"\"\"\n",
    "    return(encoded_seqs[..., ::-1, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       GGAGATTTGTGTATGTACTCTTCTTTCACGCATATGTGTGAGCAAA...\n",
       "1       AGCCAGGCCCCCAAAATAACTTGCCAGATATGTCACCTGCTTCCCA...\n",
       "2       ATCAGTGGAAATTTAAGAAAATACACATGGCCAGGCCCCAGCCCAA...\n",
       "3       TTGTCCTGAATCGCCAGATTCAGGAGGCATAAAAACCAAAATAGAG...\n",
       "4       AGAGAAGCAGCAGGACAGAGAGTGAGAGAAGGGGAGGGAGCAAAAG...\n",
       "                              ...                        \n",
       "2495    ATTAACTCAAGAATATACTGCTTCCTTGTTCTCTCCTTCTTGCCCC...\n",
       "2496    AAAGTAGGCGGTTAGCCAGGCATGGTGTTGCACGCCTGTAGTCCCA...\n",
       "2497    GGCTCTTATTTGGCCGGAGTGGAGTGACCAGGTCAGCGCCGCAGCT...\n",
       "2498    CACCGGGGGTCCTCGAAGCGCACGAAGGCGAAGGGCACGAGGCCGT...\n",
       "2499    TGGGGTGGGGGGCGTCGTCCCGTGGTGGCGCCGGCCGGGGTGGGGG...\n",
       "Length: 2500, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source of the example sequences:\n",
    "# https://github.com/kundajelab/dragonn/tree/master/examples\n",
    "# Parse FASTA sequences\n",
    "pos_seqs = {}\n",
    "neg_seqs = {}\n",
    "#with gzip.open(\"../Data/pos_seqs.fa.gz\", \"rt\") as handle:\n",
    "with gzip.open(\"./examples/example_pos_sequences.fa.gz\", \"rt\") as handle:\n",
    "    for seq_record in SeqIO.parse(handle, \"fasta\"):\n",
    "        pos_seqs[seq_record.id] = str(seq_record.seq).upper()\n",
    "pos_seqs = pd.Series(pos_seqs)\n",
    "#with gzip.open(\"../Data/neg_seqs.fa.gz\", \"rt\") as handle:\n",
    "with gzip.open(\"./examples/example_neg_sequences.fa.gz\", \"rt\") as handle:\n",
    "    for seq_record in SeqIO.parse(handle, \"fasta\"):\n",
    "        neg_seqs[seq_record.id] = str(seq_record.seq).upper()\n",
    "neg_seqs = pd.Series(neg_seqs)\n",
    "pos_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 1.],\n",
       "        [0., 1., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 1., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 1., 0., 1.],\n",
       "        [1., 1., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 1., 1.],\n",
       "        [1., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 1., 1., 0.],\n",
       "        [0., 1., 1., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]]], dtype=float16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode sequences\n",
    "pos_seqs_1_hot = pos_seqs.map(lambda x: one_hot_encode(x))\n",
    "pos_seqs_1_hot = np.stack(pos_seqs_1_hot, axis=0)\n",
    "neg_seqs_1_hot = neg_seqs.map(lambda x: one_hot_encode(x))\n",
    "neg_seqs_1_hot = np.stack(neg_seqs_1_hot, axis=0)\n",
    "pos_seqs_1_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 1.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 1.],\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 1., ..., 1., 1., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split sequences into train, validation and test\n",
    "seed = 123\n",
    "pos_train_seqs, pos_test_seqs = train_test_split(\n",
    "    pos_seqs_1_hot, test_size=0.2, random_state=seed\n",
    ")\n",
    "pos_validation_seqs, pos_test_seqs = train_test_split(\n",
    "    pos_test_seqs, test_size=0.5, random_state=seed\n",
    ")\n",
    "neg_train_seqs, neg_test_seqs = train_test_split(\n",
    "    neg_seqs_1_hot, test_size=0.2, random_state=seed\n",
    ")\n",
    "neg_validation_seqs, neg_test_seqs = train_test_split(\n",
    "    neg_test_seqs, test_size=0.5, random_state=seed\n",
    ")\n",
    "pos_train_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 1., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 1., 1.]],\n",
       "\n",
       "       [[0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 1.],\n",
       "        [1., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 1., ..., 1., 0., 0.]]], dtype=float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reverse complement train sequences\n",
    "pos_train_seqs_rc = np.append(pos_train_seqs, reverse_complement(pos_train_seqs), axis=0)\n",
    "neg_train_seqs_rc = np.append(neg_train_seqs, reverse_complement(pos_train_seqs), axis=0)\n",
    "pos_train_seqs_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7f93585e6370>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create TensorDatasets\n",
    "X = np.concatenate((pos_train_seqs_rc, neg_train_seqs_rc))\n",
    "y = np.concatenate(\n",
    "    (np.ones((len(pos_train_seqs_rc), 1)), np.zeros((len(neg_train_seqs_rc), 1)))\n",
    ")\n",
    "train_dataset = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "X = np.concatenate((pos_validation_seqs, neg_validation_seqs))\n",
    "y = np.concatenate(\n",
    "    (np.ones((len(pos_validation_seqs), 1)), np.zeros((len(neg_validation_seqs), 1)))\n",
    ")\n",
    "validation_dataset = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "X = np.concatenate((pos_test_seqs, neg_test_seqs))\n",
    "y = np.concatenate(\n",
    "    (np.ones((len(pos_test_seqs), 1)), np.zeros((len(neg_test_seqs), 1)))\n",
    ")\n",
    "test_dataset = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f92ce8e9d00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "parameters = dict(batch_size=64, shuffle=True, num_workers=8)\n",
    "train_dataloader = DataLoader(train_dataset, **parameters)\n",
    "validation_dataloader = DataLoader(validation_dataset, **parameters)\n",
    "test_dataloader = DataLoader(test_dataset, **parameters)\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/100] train_loss: 1.12768 (25.466 sec) valid_loss: 0.69347 (0.524 sec)\n",
      "Validation loss decreased (inf --> 0.693471), saving model ...\n",
      "[  2/100] train_loss: 0.69349 (25.487 sec) valid_loss: 0.69286 (0.530 sec)\n",
      "Validation loss decreased (0.693471 --> 0.692862), saving model ...\n",
      "[  3/100] train_loss: 0.69316 (25.475 sec) valid_loss: 0.69287 (0.522 sec)\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[  4/100] train_loss: 0.69340 (25.475 sec) valid_loss: 0.69305 (0.521 sec)\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[  5/100] train_loss: 0.69319 (25.467 sec) valid_loss: 0.69313 (0.522 sec)\n",
      "EarlyStopping counter: 3 out of 20\n",
      "[  6/100] train_loss: 0.69311 (25.459 sec) valid_loss: 0.69278 (0.520 sec)\n",
      "Validation loss decreased (0.692862 --> 0.692784), saving model ...\n",
      "[  7/100] train_loss: 0.69274 (25.521 sec) valid_loss: 0.69101 (0.523 sec)\n",
      "Validation loss decreased (0.692784 --> 0.691014), saving model ...\n",
      "[  8/100] train_loss: 0.69009 (25.553 sec) valid_loss: 0.67112 (0.529 sec)\n",
      "Validation loss decreased (0.691014 --> 0.671117), saving model ...\n",
      "[  9/100] train_loss: 0.68251 (25.499 sec) valid_loss: 0.65967 (0.521 sec)\n",
      "Validation loss decreased (0.671117 --> 0.659667), saving model ...\n",
      "[ 10/100] train_loss: 0.67793 (25.489 sec) valid_loss: 0.67170 (0.524 sec)\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 11/100] train_loss: 0.65805 (25.459 sec) valid_loss: 0.57003 (0.528 sec)\n",
      "Validation loss decreased (0.659667 --> 0.570034), saving model ...\n",
      "[ 12/100] train_loss: 0.62815 (25.472 sec) valid_loss: 0.50757 (0.524 sec)\n",
      "Validation loss decreased (0.570034 --> 0.507574), saving model ...\n",
      "[ 13/100] train_loss: 0.59643 (25.496 sec) valid_loss: 0.45673 (0.528 sec)\n",
      "Validation loss decreased (0.507574 --> 0.456729), saving model ...\n",
      "[ 14/100] train_loss: 0.57475 (25.472 sec) valid_loss: 0.46231 (0.522 sec)\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 15/100] train_loss: 0.54711 (25.470 sec) valid_loss: 0.42750 (0.526 sec)\n",
      "Validation loss decreased (0.456729 --> 0.427497), saving model ...\n",
      "[ 16/100] train_loss: 0.52837 (25.487 sec) valid_loss: 0.45285 (0.523 sec)\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 17/100] train_loss: 0.51493 (25.469 sec) valid_loss: 0.44414 (0.523 sec)\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 18/100] train_loss: 0.49564 (25.473 sec) valid_loss: 0.42774 (0.530 sec)\n",
      "EarlyStopping counter: 3 out of 20\n",
      "[ 19/100] train_loss: 0.48390 (25.469 sec) valid_loss: 0.45412 (0.526 sec)\n",
      "EarlyStopping counter: 4 out of 20\n",
      "[ 20/100] train_loss: 0.47280 (25.473 sec) valid_loss: 0.41440 (0.527 sec)\n",
      "Validation loss decreased (0.427497 --> 0.414399), saving model ...\n",
      "[ 21/100] train_loss: 0.45921 (25.483 sec) valid_loss: 0.41480 (0.523 sec)\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 22/100] train_loss: 0.45367 (25.462 sec) valid_loss: 0.42095 (0.522 sec)\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 23/100] train_loss: 0.44388 (25.463 sec) valid_loss: 0.39222 (0.524 sec)\n",
      "Validation loss decreased (0.414399 --> 0.392224), saving model ...\n",
      "[ 24/100] train_loss: 0.43788 (25.483 sec) valid_loss: 0.43058 (0.527 sec)\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 25/100] train_loss: 0.42665 (25.465 sec) valid_loss: 0.45569 (0.523 sec)\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 26/100] train_loss: 0.41679 (25.472 sec) valid_loss: 0.38814 (0.523 sec)\n",
      "Validation loss decreased (0.392224 --> 0.388144), saving model ...\n",
      "[ 27/100] train_loss: 0.41236 (25.480 sec) valid_loss: 0.44357 (0.522 sec)\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 28/100] train_loss: 0.41093 (25.475 sec) valid_loss: 0.42574 (0.533 sec)\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 29/100] train_loss: 0.40335 (25.475 sec) valid_loss: 0.45340 (0.532 sec)\n",
      "EarlyStopping counter: 3 out of 20\n",
      "[ 30/100] train_loss: 0.39945 (25.460 sec) valid_loss: 0.43641 (0.526 sec)\n",
      "EarlyStopping counter: 4 out of 20\n",
      "[ 31/100] train_loss: 0.39501 (25.469 sec) valid_loss: 0.44047 (0.531 sec)\n",
      "EarlyStopping counter: 5 out of 20\n",
      "[ 32/100] train_loss: 0.39243 (25.466 sec) valid_loss: 0.41570 (0.523 sec)\n",
      "EarlyStopping counter: 6 out of 20\n",
      "[ 33/100] train_loss: 0.38917 (25.448 sec) valid_loss: 0.51404 (0.523 sec)\n",
      "EarlyStopping counter: 7 out of 20\n",
      "[ 34/100] train_loss: 0.38460 (25.468 sec) valid_loss: 0.57907 (0.529 sec)\n",
      "EarlyStopping counter: 8 out of 20\n",
      "[ 35/100] train_loss: 0.38445 (25.471 sec) valid_loss: 0.39272 (0.522 sec)\n",
      "EarlyStopping counter: 9 out of 20\n",
      "[ 36/100] train_loss: 0.38328 (25.468 sec) valid_loss: 0.40233 (0.525 sec)\n",
      "EarlyStopping counter: 10 out of 20\n",
      "[ 37/100] train_loss: 0.38013 (25.473 sec) valid_loss: 0.45197 (0.522 sec)\n",
      "Epoch    37: reducing learning rate of group 0 to 5.0000e-04.\n",
      "EarlyStopping counter: 11 out of 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-20edef1f8663>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Keep the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mt_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train and validate\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_epochs = 100\n",
    "output_dir = \"./examples/model/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize model, criterion, optimizer\n",
    "model = DanQ(len(pos_seqs[0]), 1).to(device)\n",
    "criterion = get_criterion()\n",
    "optimizer = get_optimizer(model.parameters())\n",
    "# scheduler = ReduceLROnPlateau(optimizer, \"min\", patience=10, verbose=True, factor=0.5)\n",
    "state_dict = os.path.join(output_dir, \"model.pth.tar\")\n",
    "early_stopping = EarlyStopping(20, True, path=state_dict)\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "\n",
    "    # Train\n",
    "    t_time = time()\n",
    "    model.train() # set the model in train mode\n",
    "    train_losses.append([])\n",
    "    for seqs, labels in train_dataloader:\n",
    "        x = seqs.to(device) # shape = (batch_size, 4, 200)\n",
    "        labels = labels.to(device)\n",
    "        # Zero existing gradients so they don't add up\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, labels) \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Keep the loss\n",
    "        train_losses[-1].append(loss.item())\n",
    "    t_loss = np.average(train_losses[-1])\n",
    "    t_time = time() - t_time\n",
    "\n",
    "    # Validate\n",
    "    v_time = time()\n",
    "    model.eval() # set the model in evaluation mode\n",
    "    validation_losses.append([])\n",
    "    for seqs, labels in validation_dataloader:\n",
    "        x = seqs.to(device) # shape = (batch_size, 4, 200)\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, labels) \n",
    "            # Keep the loss\n",
    "            validation_losses[-1].append(loss.item())\n",
    "    v_loss = np.average(validation_losses[-1])\n",
    "    v_time = time() - v_time\n",
    "\n",
    "    print(f'[{epoch:>{3}}/{max_epochs:>{3}}] '\n",
    "         +f'train_loss: {t_loss:.5f} ({t_time:.3f} sec) '\n",
    "         +f'valid_loss: {v_loss:.5f} ({v_time:.3f} sec)')\n",
    "\n",
    "    # # Adjust learning rate\n",
    "    # scheduler.step(math.ceil(v_loss * 1000.0) / 1000.0)\n",
    "\n",
    "    # EarlyStopping needs to check if the validation loss has decresed, \n",
    "    # and if it has, it will save the current model.\n",
    "    early_stopping(v_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        # Empty cache\n",
    "        with torch.cuda.device(device):\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"Stop!!!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.41261649e-03],\n",
       "       [8.27941418e-01],\n",
       "       [9.99908805e-01],\n",
       "       [4.03591692e-02],\n",
       "       [3.12450588e-01],\n",
       "       [4.07138795e-01],\n",
       "       [1.29629960e-02],\n",
       "       [4.47772117e-03],\n",
       "       [9.99733508e-01],\n",
       "       [2.50003394e-02],\n",
       "       [3.01574767e-01],\n",
       "       [7.30029225e-01],\n",
       "       [9.48388502e-03],\n",
       "       [4.35647994e-01],\n",
       "       [2.25777894e-01],\n",
       "       [7.05034360e-02],\n",
       "       [9.93765593e-01],\n",
       "       [5.13630390e-01],\n",
       "       [3.70470583e-02],\n",
       "       [7.81637728e-01],\n",
       "       [9.96638656e-01],\n",
       "       [1.93989560e-01],\n",
       "       [5.20085752e-01],\n",
       "       [9.35907662e-01],\n",
       "       [4.16361727e-02],\n",
       "       [1.26960963e-01],\n",
       "       [9.95359600e-01],\n",
       "       [9.95212674e-01],\n",
       "       [6.57874525e-01],\n",
       "       [2.19868682e-02],\n",
       "       [5.83609641e-01],\n",
       "       [7.72888422e-01],\n",
       "       [1.74071923e-01],\n",
       "       [5.55980384e-01],\n",
       "       [9.86062229e-01],\n",
       "       [3.70097198e-02],\n",
       "       [6.21218383e-01],\n",
       "       [4.91056085e-01],\n",
       "       [9.99880314e-01],\n",
       "       [5.50869107e-01],\n",
       "       [8.08041871e-01],\n",
       "       [2.30352044e-01],\n",
       "       [4.98002797e-01],\n",
       "       [5.73941529e-01],\n",
       "       [9.61429179e-02],\n",
       "       [9.57906339e-03],\n",
       "       [2.12300465e-01],\n",
       "       [3.25503588e-01],\n",
       "       [1.60741732e-01],\n",
       "       [9.31043625e-01],\n",
       "       [9.74717498e-01],\n",
       "       [5.08679748e-01],\n",
       "       [1.85668960e-01],\n",
       "       [2.83136547e-01],\n",
       "       [4.86070454e-01],\n",
       "       [9.94413853e-01],\n",
       "       [9.99824941e-01],\n",
       "       [9.98721778e-01],\n",
       "       [8.12587500e-01],\n",
       "       [3.62721384e-01],\n",
       "       [9.22729433e-01],\n",
       "       [6.21980965e-01],\n",
       "       [2.06435964e-01],\n",
       "       [9.37814564e-02],\n",
       "       [9.96393383e-01],\n",
       "       [9.68191564e-01],\n",
       "       [9.98122990e-01],\n",
       "       [4.84634578e-01],\n",
       "       [5.69940925e-01],\n",
       "       [6.60823584e-01],\n",
       "       [9.88976121e-01],\n",
       "       [4.42270219e-01],\n",
       "       [9.99343693e-01],\n",
       "       [1.45968504e-03],\n",
       "       [5.62282503e-01],\n",
       "       [2.00144336e-01],\n",
       "       [3.07423562e-01],\n",
       "       [5.51117808e-02],\n",
       "       [5.31638622e-01],\n",
       "       [5.62204160e-02],\n",
       "       [8.18923290e-04],\n",
       "       [4.88797516e-01],\n",
       "       [9.47155118e-01],\n",
       "       [4.60063219e-02],\n",
       "       [5.20316511e-02],\n",
       "       [7.05236197e-01],\n",
       "       [9.72382963e-01],\n",
       "       [7.07559288e-01],\n",
       "       [6.83248281e-01],\n",
       "       [5.92783689e-01],\n",
       "       [5.81850648e-01],\n",
       "       [5.54647408e-02],\n",
       "       [4.31807011e-01],\n",
       "       [8.04967821e-01],\n",
       "       [2.84908656e-02],\n",
       "       [6.52992949e-02],\n",
       "       [5.06381989e-01],\n",
       "       [4.20493990e-01],\n",
       "       [1.30019365e-02],\n",
       "       [3.65294144e-02],\n",
       "       [5.65498710e-01],\n",
       "       [7.55384415e-02],\n",
       "       [1.70302972e-01],\n",
       "       [9.99925494e-01],\n",
       "       [1.01546738e-02],\n",
       "       [7.61585087e-02],\n",
       "       [1.58373769e-02],\n",
       "       [5.50344884e-01],\n",
       "       [4.92214918e-01],\n",
       "       [9.92991447e-01],\n",
       "       [8.93235147e-01],\n",
       "       [9.83645320e-01],\n",
       "       [2.87662484e-02],\n",
       "       [4.75640818e-02],\n",
       "       [1.34879798e-01],\n",
       "       [9.99023318e-01],\n",
       "       [5.15010115e-03],\n",
       "       [2.30850384e-01],\n",
       "       [5.37925363e-01],\n",
       "       [5.66317141e-01],\n",
       "       [2.80661404e-01],\n",
       "       [1.20729417e-01],\n",
       "       [5.33746123e-01],\n",
       "       [2.55746722e-01],\n",
       "       [9.99902010e-01],\n",
       "       [9.29192841e-01],\n",
       "       [1.59266181e-02],\n",
       "       [9.46702123e-01],\n",
       "       [9.99962807e-01],\n",
       "       [2.03178786e-02],\n",
       "       [1.19708858e-01],\n",
       "       [9.97822165e-01],\n",
       "       [2.93337733e-01],\n",
       "       [9.99971747e-01],\n",
       "       [6.11952603e-01],\n",
       "       [2.82473773e-01],\n",
       "       [5.33616424e-01],\n",
       "       [8.67353201e-01],\n",
       "       [4.30699050e-01],\n",
       "       [9.50355113e-01],\n",
       "       [5.26874840e-01],\n",
       "       [3.81692469e-01],\n",
       "       [2.19741417e-03],\n",
       "       [5.57945430e-01],\n",
       "       [5.32022631e-03],\n",
       "       [5.50065458e-01],\n",
       "       [3.77169937e-01],\n",
       "       [9.92085397e-01],\n",
       "       [9.98729646e-01],\n",
       "       [6.49062619e-02],\n",
       "       [9.96135235e-01],\n",
       "       [9.11646366e-01],\n",
       "       [8.47623527e-01],\n",
       "       [6.08104825e-01],\n",
       "       [7.34561861e-01],\n",
       "       [9.10946369e-01],\n",
       "       [7.52241388e-02],\n",
       "       [4.97597843e-01],\n",
       "       [2.05336534e-03],\n",
       "       [7.09432840e-01],\n",
       "       [4.91232306e-01],\n",
       "       [8.36932361e-01],\n",
       "       [7.96889141e-03],\n",
       "       [8.01030733e-03],\n",
       "       [6.13671303e-01],\n",
       "       [9.58861768e-01],\n",
       "       [5.15760899e-01],\n",
       "       [9.33510840e-01],\n",
       "       [8.19060951e-03],\n",
       "       [6.39447272e-02],\n",
       "       [6.50448024e-01],\n",
       "       [3.00788997e-06],\n",
       "       [9.81167912e-01],\n",
       "       [2.42687133e-03],\n",
       "       [3.56043369e-01],\n",
       "       [6.07952327e-02],\n",
       "       [7.53973961e-01],\n",
       "       [9.99777019e-01],\n",
       "       [3.15945357e-01],\n",
       "       [9.59027767e-01],\n",
       "       [5.31296015e-01],\n",
       "       [9.98070419e-01],\n",
       "       [1.11532817e-02],\n",
       "       [9.81250107e-01],\n",
       "       [5.12230873e-01],\n",
       "       [4.24881250e-01],\n",
       "       [5.72837770e-01],\n",
       "       [1.36550970e-03],\n",
       "       [9.63088870e-01],\n",
       "       [1.49067432e-01],\n",
       "       [5.33805072e-01],\n",
       "       [2.69244164e-01],\n",
       "       [9.60682094e-01],\n",
       "       [5.69217324e-01],\n",
       "       [4.33876878e-04],\n",
       "       [9.77810502e-01],\n",
       "       [1.91877526e-03],\n",
       "       [3.58277000e-03],\n",
       "       [5.15838247e-03],\n",
       "       [7.31529236e-01],\n",
       "       [5.14647305e-01],\n",
       "       [9.89459395e-01],\n",
       "       [7.37834394e-01],\n",
       "       [1.91814359e-03],\n",
       "       [7.13888764e-01],\n",
       "       [4.95891094e-01],\n",
       "       [6.70217024e-03],\n",
       "       [1.77612260e-01],\n",
       "       [7.75329828e-01],\n",
       "       [5.44675469e-01],\n",
       "       [8.23264048e-02],\n",
       "       [9.97681737e-01],\n",
       "       [4.93130893e-01],\n",
       "       [8.89954448e-01],\n",
       "       [1.79675147e-02],\n",
       "       [1.30225644e-01],\n",
       "       [8.95009041e-01],\n",
       "       [3.48059013e-02],\n",
       "       [6.97091827e-03],\n",
       "       [1.41900018e-01],\n",
       "       [6.61895931e-01],\n",
       "       [5.52967310e-01],\n",
       "       [8.93244863e-01],\n",
       "       [9.97346997e-01],\n",
       "       [3.43086710e-03],\n",
       "       [3.84244486e-04],\n",
       "       [7.57600546e-01],\n",
       "       [4.64342395e-03],\n",
       "       [5.59810102e-01],\n",
       "       [2.16781467e-01],\n",
       "       [2.14622393e-01],\n",
       "       [4.36763316e-02],\n",
       "       [6.41503930e-01],\n",
       "       [6.23206235e-03],\n",
       "       [5.49566388e-01],\n",
       "       [3.84166762e-02],\n",
       "       [2.38861680e-01],\n",
       "       [7.21413195e-02],\n",
       "       [2.91017890e-01],\n",
       "       [5.02642870e-01],\n",
       "       [9.45200264e-01],\n",
       "       [9.99647737e-01],\n",
       "       [3.44694555e-01],\n",
       "       [4.43576068e-01],\n",
       "       [7.59218354e-03],\n",
       "       [3.90577018e-01],\n",
       "       [4.85487938e-01],\n",
       "       [1.95297733e-01],\n",
       "       [8.72982621e-01],\n",
       "       [5.11109293e-01],\n",
       "       [8.02594900e-01],\n",
       "       [8.68517637e-01],\n",
       "       [9.99962807e-01],\n",
       "       [5.60064148e-03],\n",
       "       [9.74687815e-01],\n",
       "       [2.91815817e-01],\n",
       "       [1.39190387e-02],\n",
       "       [2.54944831e-01],\n",
       "       [4.32730466e-03],\n",
       "       [1.98653728e-01],\n",
       "       [7.43881688e-02],\n",
       "       [6.24807119e-01],\n",
       "       [4.59107071e-01],\n",
       "       [5.01091540e-01],\n",
       "       [6.53243303e-01],\n",
       "       [9.64862466e-01],\n",
       "       [3.00268143e-01],\n",
       "       [4.03616428e-02],\n",
       "       [1.74550749e-02],\n",
       "       [5.42865098e-01],\n",
       "       [1.04072496e-01],\n",
       "       [1.03683975e-02],\n",
       "       [8.77756774e-01],\n",
       "       [9.86326575e-01],\n",
       "       [9.95273530e-01],\n",
       "       [5.62846303e-01],\n",
       "       [9.89128411e-01],\n",
       "       [1.96508337e-02],\n",
       "       [1.39302644e-03],\n",
       "       [1.08507986e-03],\n",
       "       [5.09793997e-01],\n",
       "       [4.34238195e-01],\n",
       "       [6.30621538e-02],\n",
       "       [1.09840378e-01],\n",
       "       [9.03101385e-01],\n",
       "       [3.71064782e-01],\n",
       "       [4.97851461e-01],\n",
       "       [2.74331599e-01],\n",
       "       [3.10670465e-01],\n",
       "       [8.60350549e-01],\n",
       "       [1.18997563e-02],\n",
       "       [3.87099147e-01],\n",
       "       [5.08917868e-01],\n",
       "       [4.98128057e-01],\n",
       "       [8.78881216e-02],\n",
       "       [4.82940018e-01],\n",
       "       [6.23892136e-02],\n",
       "       [3.12668204e-01],\n",
       "       [6.81371331e-01],\n",
       "       [9.95571971e-01],\n",
       "       [5.04503548e-01],\n",
       "       [1.47961721e-01],\n",
       "       [9.99611318e-01],\n",
       "       [4.54417318e-01],\n",
       "       [4.71048295e-01],\n",
       "       [9.99870896e-01],\n",
       "       [9.74152803e-01],\n",
       "       [7.67714083e-02],\n",
       "       [9.98985946e-01],\n",
       "       [9.45759639e-02],\n",
       "       [6.69033289e-01],\n",
       "       [4.42717433e-01],\n",
       "       [7.96228945e-01],\n",
       "       [5.40823042e-01],\n",
       "       [1.23418629e-01],\n",
       "       [3.17924172e-02],\n",
       "       [5.52465260e-01],\n",
       "       [8.54767188e-02],\n",
       "       [8.28392088e-01],\n",
       "       [1.44793289e-02],\n",
       "       [7.04464793e-01],\n",
       "       [5.75768471e-01],\n",
       "       [6.72224760e-01],\n",
       "       [3.55747826e-02],\n",
       "       [8.36671472e-01],\n",
       "       [9.99947548e-01],\n",
       "       [1.46477968e-02],\n",
       "       [9.99630451e-01],\n",
       "       [8.69111359e-01],\n",
       "       [9.80037570e-01],\n",
       "       [4.02788162e-01],\n",
       "       [1.32393874e-02],\n",
       "       [5.28156817e-01],\n",
       "       [2.28154317e-01],\n",
       "       [8.90786052e-01],\n",
       "       [3.74827348e-02],\n",
       "       [4.97515142e-01],\n",
       "       [5.58390543e-02],\n",
       "       [7.82758221e-02],\n",
       "       [6.55308247e-01],\n",
       "       [1.66921437e-01],\n",
       "       [3.09293747e-01],\n",
       "       [6.16862416e-01],\n",
       "       [9.85301077e-01],\n",
       "       [2.22751275e-01],\n",
       "       [9.89440754e-02],\n",
       "       [3.17889615e-03],\n",
       "       [4.26814467e-01],\n",
       "       [9.17242885e-01],\n",
       "       [9.12246644e-01],\n",
       "       [5.66199601e-01],\n",
       "       [9.99422431e-01],\n",
       "       [9.66439173e-02],\n",
       "       [4.15615052e-01],\n",
       "       [4.20307228e-03],\n",
       "       [2.05082875e-02],\n",
       "       [2.76476407e-04],\n",
       "       [4.50736821e-01],\n",
       "       [8.96522462e-01],\n",
       "       [9.54002202e-01],\n",
       "       [9.99556959e-01],\n",
       "       [8.01511258e-02],\n",
       "       [9.98902798e-01],\n",
       "       [5.33573568e-01],\n",
       "       [1.80487022e-01],\n",
       "       [3.12991351e-01],\n",
       "       [9.39506590e-01],\n",
       "       [3.97443846e-02],\n",
       "       [2.34536957e-02],\n",
       "       [4.09878105e-01],\n",
       "       [8.72888938e-02],\n",
       "       [9.98739660e-01],\n",
       "       [1.22081721e-02],\n",
       "       [2.22138330e-01],\n",
       "       [9.86528099e-01],\n",
       "       [3.22540313e-01],\n",
       "       [9.99977112e-01],\n",
       "       [9.76699829e-01],\n",
       "       [6.47465050e-01],\n",
       "       [5.27212620e-02],\n",
       "       [1.59101270e-04],\n",
       "       [1.76658249e-03],\n",
       "       [5.06308913e-01],\n",
       "       [3.61248255e-01],\n",
       "       [8.22951198e-01],\n",
       "       [9.99838471e-01],\n",
       "       [5.99945858e-02],\n",
       "       [6.44985810e-02],\n",
       "       [3.32338139e-02],\n",
       "       [1.62292086e-02],\n",
       "       [5.35138071e-01],\n",
       "       [5.45960188e-01],\n",
       "       [1.86869071e-03],\n",
       "       [2.24286541e-01],\n",
       "       [9.77729499e-01],\n",
       "       [5.26565909e-01],\n",
       "       [5.49184859e-01],\n",
       "       [2.73124930e-02],\n",
       "       [9.67371464e-01],\n",
       "       [9.99594986e-01],\n",
       "       [8.39158058e-01],\n",
       "       [8.45134318e-01],\n",
       "       [1.36407107e-01],\n",
       "       [9.97309804e-01],\n",
       "       [5.05207121e-01],\n",
       "       [4.97489721e-01],\n",
       "       [1.93695612e-02],\n",
       "       [3.97071093e-01],\n",
       "       [3.26450542e-02],\n",
       "       [4.33642119e-01],\n",
       "       [4.76398282e-02],\n",
       "       [3.47658753e-01],\n",
       "       [6.07927322e-01],\n",
       "       [1.88351884e-01],\n",
       "       [5.65645039e-01],\n",
       "       [9.88049686e-01],\n",
       "       [5.14914215e-01],\n",
       "       [1.95957813e-03],\n",
       "       [8.71275902e-01],\n",
       "       [5.12491465e-01],\n",
       "       [7.76345789e-01],\n",
       "       [3.50976922e-02],\n",
       "       [9.00803745e-01],\n",
       "       [7.43825436e-01],\n",
       "       [1.95476189e-01],\n",
       "       [4.99777883e-01],\n",
       "       [5.82573056e-01],\n",
       "       [6.12920165e-01],\n",
       "       [8.22592080e-01],\n",
       "       [1.62833840e-01],\n",
       "       [1.00606783e-04],\n",
       "       [5.82957864e-01],\n",
       "       [5.47422841e-02],\n",
       "       [6.10519469e-01],\n",
       "       [9.78368163e-01],\n",
       "       [5.50402761e-01],\n",
       "       [2.39039958e-01],\n",
       "       [3.33678089e-02],\n",
       "       [4.73756082e-02],\n",
       "       [2.87196606e-01],\n",
       "       [7.16907263e-01],\n",
       "       [3.24788988e-02],\n",
       "       [4.04436199e-04],\n",
       "       [4.83636856e-01],\n",
       "       [3.06181192e-01],\n",
       "       [2.03812122e-01],\n",
       "       [1.95567235e-01],\n",
       "       [3.84486347e-01],\n",
       "       [3.74385536e-01],\n",
       "       [3.90118174e-02],\n",
       "       [9.88441467e-01],\n",
       "       [9.98505473e-01],\n",
       "       [1.03859347e-03],\n",
       "       [9.07293618e-01],\n",
       "       [4.60078031e-01],\n",
       "       [8.30412377e-03],\n",
       "       [9.99925494e-01],\n",
       "       [8.28980565e-01],\n",
       "       [9.15657058e-02],\n",
       "       [4.90422845e-01],\n",
       "       [5.02540588e-01],\n",
       "       [9.76028621e-01],\n",
       "       [9.49126840e-01],\n",
       "       [8.20845723e-01],\n",
       "       [9.99863029e-01],\n",
       "       [9.99031425e-01],\n",
       "       [9.98527288e-01],\n",
       "       [3.87140140e-02],\n",
       "       [9.99178827e-01],\n",
       "       [4.72281337e-01],\n",
       "       [5.13035394e-02],\n",
       "       [8.60247165e-02],\n",
       "       [1.67397350e-01],\n",
       "       [5.44786811e-01],\n",
       "       [9.99961138e-01],\n",
       "       [5.83425015e-02],\n",
       "       [4.49684292e-01],\n",
       "       [4.19812173e-01],\n",
       "       [6.30874693e-01],\n",
       "       [7.07800448e-01],\n",
       "       [5.10771036e-01],\n",
       "       [3.67096364e-01],\n",
       "       [7.07509145e-02],\n",
       "       [5.02656460e-01],\n",
       "       [5.09716153e-01],\n",
       "       [9.69179809e-01],\n",
       "       [2.91287094e-01],\n",
       "       [1.68621149e-02],\n",
       "       [3.74283969e-01],\n",
       "       [2.25220304e-02],\n",
       "       [9.31842268e-01],\n",
       "       [6.81385517e-01],\n",
       "       [9.98520672e-01],\n",
       "       [9.99859452e-01],\n",
       "       [3.83874089e-01],\n",
       "       [6.13626897e-01],\n",
       "       [9.63164926e-01],\n",
       "       [2.44141698e-01],\n",
       "       [3.98876905e-01],\n",
       "       [4.15068008e-02]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "y = None\n",
    "predictions = None\n",
    "state_dict = os.path.join(output_dir, \"model.pth.tar\")\n",
    "model.load_state_dict(torch.load(state_dict))\n",
    "\n",
    "model.eval() # set the model in evaluation mode\n",
    "for seqs, labels in test_dataloader:\n",
    "    x = seqs.to(device) # shape = (batch_size, 4, 200)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "        # Save predictions\n",
    "        if predictions is None and y is None:\n",
    "            predictions = outputs.data.cpu().numpy()\n",
    "            y = labels.data.cpu().numpy()\n",
    "        else:\n",
    "            predictions = np.append(\n",
    "                predictions, outputs.data.cpu().numpy(), axis=0\n",
    "            )\n",
    "            y = np.append(y, labels.data.cpu().numpy(), axis=0)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final performance metrics: AUCROC: 0.89542, AUCPR: 0.88929, MCC: 0.61649\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "metrics = dict(AUCPR=None, AUCROC=None, MCC=None)\n",
    "p = predictions.flatten()\n",
    "l = y.flatten()\n",
    "\n",
    "# Plot\n",
    "def __visualize_metric(data, columns, metric, score):\n",
    "\n",
    "    # Metric to DataFrame\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    # Seaborn aesthetics\n",
    "    sns.set_context(\"paper\", font_scale=1.5, rc={\"lines.linewidth\": 1.5})\n",
    "    sns.set_palette(sns.color_palette([\"#1965B0\"]))\n",
    "    # Plot metric\n",
    "    kwargs = dict(estimator=None, ci=None)\n",
    "    g = sns.lineplot(x=columns[0], y=columns[1], data=df, **kwargs)\n",
    "    # Add metric score\n",
    "    kwargs = dict(horizontalalignment=\"center\", verticalalignment=\"center\")\n",
    "    plt.text(.5, 0, \"%s = %.5f\" % (metric, score), **kwargs)\n",
    "    # Remove spines\n",
    "    sns.despine()\n",
    "    # Save & close\n",
    "    fig = g.get_figure()\n",
    "    fig.savefig(os.path.join(output_dir, \"%s.png\" % metric))\n",
    "    plt.close(fig)\n",
    "\n",
    "# Metrics to DataFrame\n",
    "for metric in metrics:\n",
    "    if metric == \"AUCPR\":\n",
    "        score = average_precision_score(l, p)\n",
    "        prec, recall, _ = precision_recall_curve(l, p)\n",
    "        # i.e. precision = 0, recall = 1\n",
    "        prec = np.insert(prec, 0, 0., axis=0)\n",
    "        recall = np.insert(recall, 0, 1., axis=0)\n",
    "        data = list(zip(recall, prec))\n",
    "        __visualize_metric(data, [\"Recall\", \"Precision\"], metric, score)\n",
    "    elif metric == \"AUCROC\":\n",
    "        score = roc_auc_score(l, p)\n",
    "        fpr, tpr, _ = roc_curve(l, p)\n",
    "        data = list(zip(fpr, tpr))\n",
    "        __visualize_metric(data, [\"FPR\", \"TPR\"], metric, score)\n",
    "    elif metric == \"MCC\":\n",
    "        score = matthews_corrcoef(l, np.rint(p))\n",
    "    metrics[metric] = score\n",
    "\n",
    "print(f'Final performance metrics: '\n",
    "     +f'AUCROC: {metrics[\"AUCROC\"]:.5f}, '\n",
    "     +f'AUCPR: {metrics[\"AUCPR\"]:.5f}, '\n",
    "     +f'MCC: {metrics[\"MCC\"]:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "         1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
       "         1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "         1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
       "         1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "         1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
       "         0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "         0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
       "         0., 1., 1., 0., 0., 0., 1., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "         0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 1., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float16)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fwd = pos_train[:1]\n",
    "fwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def __visualize_metric(self, data, y, metric):\n",
    "\n",
    "    # Metric to DataFrame\n",
    "    df = pd.DataFrame(data, columns=y)\n",
    "\n",
    "    # Seaborn aesthetics\n",
    "    sns.set_context(\"paper\", font_scale=1.5, rc={\"lines.linewidth\": 1.5})\n",
    "    sns.set_palette(sns.color_palette([\"#1965B0\"]))\n",
    "\n",
    "    # Plot metric\n",
    "    kwargs = dict(estimator=None, ci=None)\n",
    "    g = sns.lineplot(x=y[0], y=y[1], data=df, **kwargs)\n",
    "\n",
    "    # Add metric score\n",
    "    kwargs = dict(horizontalalignment=\"center\", verticalalignment=\"center\")\n",
    "    plt.text(.5, 0, \"%s = %.5f\" % (metric, self.metrics[metric]), **kwargs)\n",
    "\n",
    "    # Remove spines\n",
    "    sns.despine()\n",
    "\n",
    "    # Save & close\n",
    "    fig = g.get_figure()\n",
    "    fig.savefig(os.path.join(self.output_dir, \"%s.png\" % metric))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GCATCCACACACCCTCAGATGCTTCCTTTGACGCCCTCTGCTGTGCCCCTAGACACCCCTATCCCGCCACTGGCTGAAGCTGGACTTTGGAGCCATCTGCCTCCCTTGCCTGCGTCCACACCCCGCGCCAGTCCTCAGCCTCCAAGCCCATCTCAGTCGGACCCTTTCTCATTCCTGCCACTCGCTGCCTGTTCCAGGCC'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_decode(fwd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
       "         0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "         0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "         1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
       "         0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
       "         1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "         0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "         1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
       "         1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "         0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "         1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "         1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
       "         1., 0., 1., 1., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "         0., 1., 0., 0., 0., 1., 0., 0.]]], dtype=float16)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev = reverse_complement_one_hot_encoding(fwd)\n",
    "rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GGCCTGGAACAGGCAGCGAGTGGCAGGAATGAGAAAGGGTCCGACTGAGATGGGCTTGGAGGCTGAGGACTGGCGCGGGGTGTGGACGCAGGCAAGGGAGGCAGATGGCTCCAAAGTCCAGCTTCAGCCAGTGGCGGGATAGGGGTGTCTAGGGGCACAGCAGAGGGCGTCAAAGGAAGCATCTGAGGGTGTGTGGATGC'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_decode(rev[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
